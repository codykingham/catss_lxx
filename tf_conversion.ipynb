{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCAT to TF Conversion\n",
    "\n",
    "In this notebook I extract the CCAT data from its plain text, online format and process it into a Text-Fabric data format. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, requests, time, glob, collections, datetime\n",
    "from tf.fabric import Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harvest Data Files from ccat.sas.upen.edu\n",
    "\n",
    "N.B. that there is a 2 second delay between each page (book) request to avoid stressing the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetch new data toggled False. Doing nothing...\n"
     ]
    }
   ],
   "source": [
    "# harvests the data from CCAT website; toggle False if already done\n",
    "request_new_data = False\n",
    "\n",
    "# data paths \n",
    "source_directory = 'source'\n",
    "tf_directory = 'tf'\n",
    "morph_dir = os.path.join(source_directory, 'morphology')\n",
    "paral_dir = os.path.join(source_directory, 'parallel')\n",
    "\n",
    "\n",
    "morph_url = 'http://ccat.sas.upenn.edu/gopher/text/religion/biblical/lxxmorph/{}'\n",
    "paral_url = 'http://ccat.sas.upenn.edu/gopher/text/religion/biblical/parallel/{}'\n",
    "\n",
    "# pasted from http://ccat.sas.upenn.edu/gopher/text/religion/biblical/lxxmorph\n",
    "morph_books = '''\n",
    "[   ]\t01.Gen.1.mlxx\t12-Jul-1994 15:10\t711K\n",
    "[   ]\t02.Gen.2.mlxx\t12-Jul-1994 15:11\t673K\n",
    "[   ]\t03.Exod.mlxx\t12-Jul-1994 15:05\t1.0M\n",
    "[   ]\t04.Lev.mlxx\t03-Aug-2015 15:13\t812K\n",
    "[   ]\t05.Num.mlxx\t10-Aug-2015 16:07\t1.0M\n",
    "[   ]\t06.Deut.mlxx\t06-Aug-2015 15:45\t1.0M\n",
    "[   ]\t07.JoshB.mlxx\t12-Jul-1994 15:55\t638K\n",
    "[   ]\t08.JoshA.mlxx\t12-Jul-1994 16:09\t46K\n",
    "[   ]\t09.JudgesB.mlxx\t12-Jul-1994 16:10\t667K\n",
    "[   ]\t10.JudgesA.mlxx\t12-Jul-1994 16:11\t683K\n",
    "[   ]\t11.Ruth.mlxx\t12-Jul-1994 16:12\t88K\n",
    "[   ]\t12.1Sam.mlxx\t04-Aug-2015 16:39\t862K\n",
    "[   ]\t13.2Sam.mlxx\t04-Aug-2015 16:43\t766K\n",
    "[   ]\t14.1Kings.mlxx\t06-Aug-2015 15:30\t888K\n",
    "[   ]\t15.2Kings.mlxx\t12-Jul-1994 16:41\t807K\n",
    "[   ]\t16.1Chron.mlxx\t12-Jul-1994 16:41\t692K\n",
    "[   ]\t17.2Chron.mlxx\t03-Aug-2015 16:30\t910K\n",
    "[   ]\t18.1Esdras.mlxx\t12-Jul-1994 16:58\t387K\n",
    "[   ]\t19.2Esdras.mlxx\t19-Jul-1994 12:45\t568K\n",
    "[   ]\t20.Esther.mlxx\t12-Jul-1994 16:59\t251K\n",
    "[   ]\t21.Judith.mlxx\t12-Jul-1994 16:59\t392K\n",
    "[   ]\t22.TobitBA.mlxx\t13-Jul-1994 09:41\t236K\n",
    "[   ]\t23.TobitS.mlxx\t13-Jul-1994 09:41\t308K\n",
    "[   ]\t24.1Macc.mlxx\t13-Jul-1994 09:42\t791K\n",
    "[   ]\t25.2Macc.mlxx\t03-Aug-2015 16:50\t519K\n",
    "[   ]\t26.3Macc.mlxx\t13-Jul-1994 09:55\t223K\n",
    "[   ]\t27.4Macc.mlxx\t19-Jul-1994 12:46\t341K\n",
    "[   ]\t28.Psalms1.mlxx\t13-Jul-1994 11:21\t752K\n",
    "[   ]\t29.Psalms2.mlxx\t06-Aug-2015 16:10\t750K\n",
    "[   ]\t30.Odes.mlxx\t13-Jul-1994 11:53\t180K\n",
    "[   ]\t31.Proverbs.mlxx\t31-Mar-2015 09:11\t490K\n",
    "[   ]\t32.Qoheleth.mlxx\t13-Jul-1994 11:54\t193K\n",
    "[   ]\t33.Canticles.mlxx\t13-Jul-1994 11:54\t87K\n",
    "[   ]\t34.Job.mlxx\t13-Jul-1994 11:54\t589K\n",
    "[   ]\t35.Wisdom.mlxx\t13-Jul-1994 12:53\t301K\n",
    "[   ]\t36.Sirach.mlxx\t10-Aug-2015 16:04\t815K\n",
    "[   ]\t37.PsSol.mlxx\t13-Jul-1994 13:36\t212K\n",
    "[   ]\t38.Hosea.mlxx\t13-Jul-1994 13:37\t170K\n",
    "[   ]\t39.Micah.mlxx\t13-Jul-1994 13:37\t102K\n",
    "[   ]\t40.Amos.mlxx\t13-Jul-1994 13:38\t138K\n",
    "[   ]\t41.Joel.mlxx\t13-Jul-1994 13:38\t68K\n",
    "[   ]\t42.Jonah.mlxx\t13-Jul-1994 13:38\t47K\n",
    "[   ]\t43.Obadiah.mlxx\t13-Jul-1994 13:40\t20K\n",
    "[   ]\t44.Nahum.mlxx\t06-Aug-2015 15:41\t40K\n",
    "[   ]\t45.Habakkuk.mlxx\t13-Jul-1994 13:41\t48K\n",
    "[   ]\t46.Zeph.mlxx\t13-Jul-1994 13:41\t53K\n",
    "[   ]\t47.Haggai.mlxx\t13-Jul-1994 13:41\t40K\n",
    "[   ]\t48.Zech.mlxx\t13-Jul-1994 13:42\t213K\n",
    "[   ]\t49.Malachi.mlxx\t13-Jul-1994 13:42\t61K\n",
    "[   ]\t50.Isaiah1.mlxx\t04-Aug-2015 16:33\t672K\n",
    "[   ]\t51.Isaiah2.mlxx\t03-Aug-2015 15:22\t485K\n",
    "[   ]\t52.Jer1.mlxx\t03-Aug-2015 15:51\t639K\n",
    "[   ]\t53.Jer2.mlxx\t06-Aug-2015 15:56\t600K\n",
    "[   ]\t54.Baruch.mlxx\t13-Jul-1994 14:27\t111K\n",
    "[   ]\t55.EpJer.mlxx\t13-Jul-1994 14:29\t56K\n",
    "[   ]\t56.Lam.mlxx\t04-Aug-2015 16:30\t105K\n",
    "[   ]\t57.Ezek1.mlxx\t13-Jul-1994 15:04\t601K\n",
    "[   ]\t58.Ezek2.mlxx\t13-Jul-1994 15:04\t663K\n",
    "[   ]\t59.BelOG.mlxx\t19-Jul-1994 12:46\t38K\n",
    "[   ]\t60.BelTh.mlxx\t19-Jul-1994 12:46\t37K\n",
    "[   ]\t61.DanielOG.mlxx\t10-Aug-2015 16:14\t460K\n",
    "[   ]\t62.DanielTh.mlxx\t19-Jul-1994 12:46\t446K\n",
    "[   ]\t63.SusOG.mlxx\t19-Jul-1994 12:46\t34K\n",
    "[   ]\t64.SusTh.mlxx\t19-Jul-1994 12:47\t49K\n",
    "'''\n",
    "\n",
    "# pasted from http://ccat.sas.upenn.edu/gopher/text/religion/biblical/parallel\n",
    "paral_books = '''\n",
    "[   ]\t01.Genesis.par\t08-Dec-1999 10:27\t379K\n",
    "[   ]\t02.Exodus.par\t05-Apr-1994 17:36\t318K\n",
    "[   ]\t03.Lev.par\t05-Apr-1994 17:36\t225K\n",
    "[   ]\t04.Num.par\t05-Apr-1994 17:36\t299K\n",
    "[   ]\t05.Deut.par\t05-Apr-1994 17:36\t265K\n",
    "[   ]\t06.JoshB.par\t05-Apr-1994 17:36\t198K\n",
    "[   ]\t07.JoshA.par\t05-Apr-1994 17:36\t13K\n",
    "[   ]\t08.JudgesB.par\t05-Apr-1994 17:36\t178K\n",
    "[   ]\t09.JudgesA.par\t02-Sep-1994 16:53\t183K\n",
    "[   ]\t10.Ruth.par\t05-Apr-1994 17:36\t23K\n",
    "[   ]\t11.1Sam.par\t05-Apr-1994 17:36\t244K\n",
    "[   ]\t12.2Sam.par\t05-Apr-1994 17:36\t209K\n",
    "[   ]\t13.1Kings.par\t05-Apr-1994 17:36\t302K\n",
    "[   ]\t14.2Kings.par\t05-Apr-1994 17:36\t220K\n",
    "[   ]\t15.1Chron.par\t05-Apr-1994 17:36\t191K\n",
    "[   ]\t16.2Chron.par\t18-Feb-2005 15:18\t242K\n",
    "[   ]\t17.1Esdras.par\t05-Apr-1994 17:36\t161K\n",
    "[   ]\t18.Esther.par\t05-Apr-1994 17:36\t84K\n",
    "[   ]\t18.Ezra.par\t05-Apr-1994 17:36\t70K\n",
    "[   ]\t19.Neh.par\t05-Apr-1994 17:36\t94K\n",
    "[   ]\t20.Psalms.par\t05-Apr-1994 17:36\t533K\n",
    "[   ]\t22.Ps151.par\t05-Apr-1994 17:36\t1.8K\n",
    "[   ]\t23.Prov.par\t05-Apr-1994 17:36\t158K\n",
    "[   ]\t24.Qoh.par\t05-Apr-1994 17:36\t48K\n",
    "[   ]\t25.Cant.par\t25-Mar-2015 11:34\t24K\n",
    "[   ]\t26.Job.par\t05-Apr-1994 17:36\t183K\n",
    "[   ]\t27.Sirach.par\t05-Apr-1994 17:36\t289K\n",
    "[   ]\t28.Hosea.par\t05-Apr-1994 17:36\t46K\n",
    "[   ]\t29.Micah.par\t05-Apr-1994 17:36\t27K\n",
    "[   ]\t30.Amos.par\t05-Apr-1994 17:36\t37K\n",
    "[   ]\t31.Joel.par\t05-Apr-1994 17:36\t18K\n",
    "[   ]\t32.Jonah.par\t05-Apr-1994 17:36\t13K\n",
    "[   ]\t33.Obadiah.par\t05-Apr-1994 17:36\t5.4K\n",
    "[   ]\t34.Nahum.par\t05-Apr-1994 17:36\t11K\n",
    "[   ]\t35.Hab.par\t05-Apr-1994 17:36\t13K\n",
    "[   ]\t36.Zeph.par\t05-Apr-1994 17:36\t14K\n",
    "[   ]\t37.Haggai.par\t05-Apr-1994 17:36\t11K\n",
    "[   ]\t38.Zech.par\t05-Apr-1994 17:36\t57K\n",
    "[   ]\t39.Malachi.par\t05-Apr-1994 17:36\t17K\n",
    "[   ]\t40.Isaiah.par\t05-Apr-1994 17:36\t334K\n",
    "[   ]\t41.Jer.par\t05-Apr-1994 17:36\t461K\n",
    "[   ]\t42.Baruch.par\t05-Apr-1994 17:36\t16K\n",
    "[   ]\t43.Lam.par\t05-Apr-1994 17:36\t30K\n",
    "[   ]\t44.Ezekiel.par\t05-Apr-1994 17:36\t359K\n",
    "[   ]\t45.DanielOG.par\t05-Apr-1994 17:36\t177K\n",
    "[   ]\t46.DanielTh.par\t05-Apr-1994 17:36\t143K\n",
    "'''\n",
    "\n",
    "# clean the book names\n",
    "morph_books = [book.split('\\t')[1] for book in morph_books.split('\\n')\n",
    "                  if book]\n",
    "paral_books = [book.split('\\t')[1] for book in paral_books.split('\\n')\n",
    "                  if book]\n",
    "\n",
    "# request the book data\n",
    "\n",
    "if request_new_data:\n",
    "    \n",
    "    # make data directory\n",
    "    if not os.path.isdir(source_directory):\n",
    "        os.mkdir(source_directory)\n",
    "    \n",
    "    # big loop, grabs per data source for every book\n",
    "    for directory, books, base_url in ((morph_dir, morph_books, morph_url), \n",
    "                                       (paral_dir, paral_books, paral_url)\n",
    "                                      ):\n",
    "        \n",
    "        # make the directory for the data type\n",
    "        if not os.path.isdir(directory):\n",
    "            os.mkdir(directory)\n",
    "        \n",
    "        print(f'harvesting data from {base_url}')\n",
    "\n",
    "        # get data per book\n",
    "        for book in books:\n",
    "            \n",
    "            book_url = base_url.format(book) # format url\n",
    "            book_file = os.path.join(directory, book) # name the book's data file\n",
    "            \n",
    "            # write to the book's data file\n",
    "            with open(book_file, 'w') as outfile:\n",
    "                print(f'\\t|writing data for {book}...')\n",
    "                outfile.write(requests.get(book_url).text) # harvest/save the data from website\n",
    "                print(f'\\t|\\tfile written...')\n",
    "                time.sleep(1) # be nice to the server and wait in between books\n",
    "        print('done!')\n",
    "else:\n",
    "    print('Fetch new data toggled False. Doing nothing...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build TF Resource\n",
    "\n",
    "Starting out the database will have have 4 objects:\n",
    "    1. word\n",
    "    2. book\n",
    "    3. chapter\n",
    "    4. verse\n",
    " \n",
    "The following dicts are needed:\n",
    "    1. otype\n",
    "    2. oslots\n",
    "    3. book\n",
    "    4. chapter\n",
    "    5. verse\n",
    "    6. trans [transcription]\n",
    "    7. morph [morphology]\n",
    " \n",
    "The morph feature is plain text and tab separated—straight from the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Book Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print out a copy/pastable link of file names to add to a dictionary\n",
    "\n",
    "if False: # change to True for printout\n",
    "    print(',\\n'.join(['\\'' + book + '\\':' for book in morph_books]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map book names (some books are split up); filenames copied/pasted from above printout\n",
    "book_names = {    \n",
    "    '01.Gen.1.mlxx':'Genesis',\n",
    "    '02.Gen.2.mlxx':'Genesis',\n",
    "    '03.Exod.mlxx':'Exodus',\n",
    "    '04.Lev.mlxx':'Leviticus',\n",
    "    '05.Num.mlxx':'Numbers',\n",
    "    '06.Deut.mlxx':'Deuteronomy',\n",
    "    '07.JoshB.mlxx':'Joshua_B',\n",
    "    '08.JoshA.mlxx':'Joshua_A',\n",
    "    '09.JudgesB.mlxx':'Judges_B',\n",
    "    '10.JudgesA.mlxx':'Judges_A',\n",
    "    '11.Ruth.mlxx':'Ruth',\n",
    "    '12.1Sam.mlxx':'1_Samuel',\n",
    "    '13.2Sam.mlxx':'2_Samuel',\n",
    "    '14.1Kings.mlxx':'1_Kings',\n",
    "    '15.2Kings.mlxx':'2_Kings',\n",
    "    '16.1Chron.mlxx':'1_Chronicles',\n",
    "    '17.2Chron.mlxx':'2_Chronicles',\n",
    "    '18.1Esdras.mlxx':'1_Esdras',\n",
    "    '19.2Esdras.mlxx':'2_Esdras',\n",
    "    '20.Esther.mlxx':'Esther',\n",
    "    '21.Judith.mlxx':'Judith',\n",
    "    '22.TobitBA.mlxx':'Tobit_BA',\n",
    "    '23.TobitS.mlxx':'Tobit_S',\n",
    "    '24.1Macc.mlxx':'1_Maccabees',\n",
    "    '25.2Macc.mlxx':'2_Maccabees',\n",
    "    '26.3Macc.mlxx':'3_Maccabees',\n",
    "    '27.4Macc.mlxx':'4_Maccabees',\n",
    "    '28.Psalms1.mlxx':'Psalms',\n",
    "    '29.Psalms2.mlxx':'Psalms',\n",
    "    '30.Odes.mlxx':'Odes',\n",
    "    '31.Proverbs.mlxx':'Proverbs',\n",
    "    '32.Qoheleth.mlxx':'Qoheleth',\n",
    "    '33.Canticles.mlxx':'Canticles',\n",
    "    '34.Job.mlxx':'Job',\n",
    "    '35.Wisdom.mlxx':'Wisdom',\n",
    "    '36.Sirach.mlxx':'Sirach',\n",
    "    '37.PsSol.mlxx':'Psalms_of_Solomon',\n",
    "    '38.Hosea.mlxx':'Hosea',\n",
    "    '39.Micah.mlxx':'Micah',\n",
    "    '40.Amos.mlxx':'Amos',\n",
    "    '41.Joel.mlxx':'Joel',\n",
    "    '42.Jonah.mlxx':'Jonah',\n",
    "    '43.Obadiah.mlxx':'Obadiah',\n",
    "    '44.Nahum.mlxx':'Nahum',\n",
    "    '45.Habakkuk.mlxx':'Habakkuk',\n",
    "    '46.Zeph.mlxx':'Zephaniah',\n",
    "    '47.Haggai.mlxx':'Haggai',\n",
    "    '48.Zech.mlxx':'Zechariah',\n",
    "    '49.Malachi.mlxx':'Malachi',\n",
    "    '50.Isaiah1.mlxx':'Isaiah',\n",
    "    '51.Isaiah2.mlxx':'Isaiah',\n",
    "    '52.Jer1.mlxx':'Jeremiah',\n",
    "    '53.Jer2.mlxx':'Jeremiah',\n",
    "    '54.Baruch.mlxx':'Baruch',\n",
    "    '55.EpJer.mlxx':'Epistle_of_Jeremiah',\n",
    "    '56.Lam.mlxx':'Lamentations',\n",
    "    '57.Ezek1.mlxx':'Ezekiel',\n",
    "    '58.Ezek2.mlxx':'Ezekiel',\n",
    "    '59.BelOG.mlxx':'Bel_and_Dragon_OG',\n",
    "    '60.BelTh.mlxx':'Bel_and_Dragon_Th',\n",
    "    '61.DanielOG.mlxx':'Daniel_OG',\n",
    "    '62.DanielTh.mlxx':'Daniel_Th',\n",
    "    '63.SusOG.mlxx':'Susanna_OG',\n",
    "    '64.SusTh.mlxx':'Susanna_Th' \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count and Map Slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing slots for 01.Gen.1.mlxx\n",
      "\t|16774 slots logged\n",
      "processing slots for 02.Gen.2.mlxx\n",
      "\t|32566 slots logged\n",
      "processing slots for 03.Exod.mlxx\n",
      "\t|24816 slots logged\n",
      "processing slots for 04.Lev.mlxx\n",
      "\t|19082 slots logged\n",
      "processing slots for 05.Num.mlxx\n",
      "\t|25059 slots logged\n",
      "processing slots for 06.Deut.mlxx\n",
      "\t|22990 slots logged\n",
      "processing slots for 07.JoshB.mlxx\n",
      "\t|14896 slots logged\n",
      "processing slots for 08.JoshA.mlxx\n",
      "\t|1064 slots logged\n",
      "processing slots for 09.JudgesB.mlxx\n",
      "\t|15580 slots logged\n",
      "processing slots for 10.JudgesA.mlxx\n",
      "\t|15947 slots logged\n",
      "processing slots for 11.Ruth.mlxx\n",
      "\t|2072 slots logged\n",
      "processing slots for 12.1Sam.mlxx\n",
      "\t|20131 slots logged\n",
      "processing slots for 13.2Sam.mlxx\n",
      "\t|17927 slots logged\n",
      "processing slots for 14.1Kings.mlxx\n",
      "\t|20803 slots logged\n",
      "processing slots for 15.2Kings.mlxx\n",
      "\t|18853 slots logged\n",
      "processing slots for 16.1Chron.mlxx\n",
      "\t|16244 slots logged\n",
      "processing slots for 17.2Chron.mlxx\n",
      "\t|21353 slots logged\n",
      "processing slots for 18.1Esdras.mlxx\n",
      "\t|8994 slots logged\n",
      "processing slots for 19.2Esdras.mlxx\n",
      "\t|13262 slots logged\n",
      "processing slots for 20.Esther.mlxx\n",
      "\t|5843 slots logged\n",
      "processing slots for 21.Judith.mlxx\n",
      "\t|9174 slots logged\n",
      "processing slots for 22.TobitBA.mlxx\n",
      "\t|5503 slots logged\n",
      "processing slots for 23.TobitS.mlxx\n",
      "\t|7233 slots logged\n",
      "processing slots for 24.1Macc.mlxx\n",
      "\t|18292 slots logged\n",
      "processing slots for 25.2Macc.mlxx\n",
      "\t|11917 slots logged\n",
      "processing slots for 26.3Macc.mlxx\n",
      "\t|5110 slots logged\n",
      "processing slots for 27.4Macc.mlxx\n",
      "\t|7859 slots logged\n",
      "processing slots for 28.Psalms1.mlxx\n",
      "\t|17518 slots logged\n",
      "processing slots for 29.Psalms2.mlxx\n",
      "\t|34964 slots logged\n",
      "processing slots for 30.Odes.mlxx\n",
      "\t|4187 slots logged\n",
      "processing slots for 31.Proverbs.mlxx\n",
      "\t|11164 slots logged\n",
      "processing slots for 32.Qoheleth.mlxx\n",
      "\t|4546 slots logged\n",
      "processing slots for 33.Canticles.mlxx\n",
      "\t|2025 slots logged\n",
      "processing slots for 34.Job.mlxx\n",
      "\t|13561 slots logged\n",
      "processing slots for 35.Wisdom.mlxx\n",
      "\t|6943 slots logged\n",
      "processing slots for 36.Sirach.mlxx\n",
      "\t|18658 slots logged\n",
      "processing slots for 37.PsSol.mlxx\n",
      "\t|4926 slots logged\n",
      "processing slots for 38.Hosea.mlxx\n",
      "\t|3933 slots logged\n",
      "processing slots for 39.Micah.mlxx\n",
      "\t|2368 slots logged\n",
      "processing slots for 40.Amos.mlxx\n",
      "\t|3210 slots logged\n",
      "processing slots for 41.Joel.mlxx\n",
      "\t|1580 slots logged\n",
      "processing slots for 42.Jonah.mlxx\n",
      "\t|1090 slots logged\n",
      "processing slots for 43.Obadiah.mlxx\n",
      "\t|472 slots logged\n",
      "processing slots for 44.Nahum.mlxx\n",
      "\t|937 slots logged\n",
      "processing slots for 45.Habakkuk.mlxx\n",
      "\t|1105 slots logged\n",
      "processing slots for 46.Zeph.mlxx\n",
      "\t|1223 slots logged\n",
      "processing slots for 47.Haggai.mlxx\n",
      "\t|947 slots logged\n",
      "processing slots for 48.Zech.mlxx\n",
      "\t|4963 slots logged\n",
      "processing slots for 49.Malachi.mlxx\n",
      "\t|1416 slots logged\n",
      "processing slots for 50.Isaiah1.mlxx\n",
      "\t|15721 slots logged\n",
      "processing slots for 51.Isaiah2.mlxx\n",
      "\t|27075 slots logged\n",
      "processing slots for 52.Jer1.mlxx\n",
      "\t|14932 slots logged\n",
      "processing slots for 53.Jer2.mlxx\n",
      "\t|28948 slots logged\n",
      "processing slots for 54.Baruch.mlxx\n",
      "\t|2608 slots logged\n",
      "processing slots for 55.EpJer.mlxx\n",
      "\t|1285 slots logged\n",
      "processing slots for 56.Lam.mlxx\n",
      "\t|2391 slots logged\n",
      "processing slots for 57.Ezek1.mlxx\n",
      "\t|14050 slots logged\n",
      "processing slots for 58.Ezek2.mlxx\n",
      "\t|29658 slots logged\n",
      "processing slots for 59.BelOG.mlxx\n",
      "\t|901 slots logged\n",
      "processing slots for 60.BelTh.mlxx\n",
      "\t|871 slots logged\n",
      "processing slots for 61.DanielOG.mlxx\n",
      "\t|10781 slots logged\n",
      "processing slots for 62.DanielTh.mlxx\n",
      "\t|10453 slots logged\n",
      "processing slots for 63.SusOG.mlxx\n",
      "\t|792 slots logged\n",
      "processing slots for 64.SusTh.mlxx\n",
      "\t|1134 slots logged\n",
      "done...\n",
      "  623685 slots created...\n",
      "  59 books created...\n",
      "  1194 chapters created...\n",
      "  30609 verses created...\n"
     ]
    }
   ],
   "source": [
    "nodes = collections.defaultdict(dict) # mapping from node to feature\n",
    "books = collections.defaultdict(list) # mapping from book to its slots\n",
    "chapters = collections.defaultdict(list) # chapter 2 slots\n",
    "verses = collections.defaultdict(list) # verse 2 slots\n",
    "\n",
    "# creat slots\n",
    "slot = 1\n",
    "for morph_book in morph_books:\n",
    "    \n",
    "    data_file = os.path.join(morph_dir, morph_book)\n",
    "    \n",
    "    print(f'processing slots for {morph_book}')\n",
    "    \n",
    "    with open(data_file, 'r') as book_data:\n",
    "        \n",
    "        this_book = book_names[morph_book]\n",
    "        \n",
    "        for line in book_data:\n",
    "            \n",
    "            data = line.strip().split()\n",
    "            \n",
    "            # length of 0/1 is either blank line or section marker with no chapter/verse label\n",
    "            if len(data) == 1 and data[0] == '':\n",
    "                continue\n",
    "            # exception for some superscriptions or in-doubt texts w/out chapter:verse label\n",
    "            elif len(data) == 1 and data[0] != '': \n",
    "                data.append('0:0') # place-holder chapter:verse\n",
    "            \n",
    "            # length of 2 is a verse marker\n",
    "            if len(data) == 2:\n",
    "                \n",
    "                # format chapter for single chapter books, e.g. Obadiah\n",
    "                if ':' not in data[1]:\n",
    "                    data[1] = '1:' + data[1]\n",
    "                \n",
    "                # assign chapter/verse\n",
    "                this_chapter, this_verse = data[1].split(':')\n",
    "                \n",
    "            # length > 2 is a slot\n",
    "            elif len(data) > 2:\n",
    "                \n",
    "                # get slot data\n",
    "                trans = data[0]\n",
    "                morph = '.'.join(data[1:]) # morpho data into dot-separated string, disambiguate later\n",
    "                \n",
    "                # save slot data\n",
    "                nodes['otype'][slot] = 'word'\n",
    "                nodes['trans'][slot] = trans\n",
    "                nodes['morph'][slot] = morph\n",
    "                nodes['trailer'][slot] = ' '# simple whitespace trailer \n",
    "                books[this_book].append(slot)\n",
    "                chapters[(this_book, this_chapter)].append(slot)\n",
    "                verses[(this_book, this_chapter, this_verse)].append(slot)\n",
    "                \n",
    "                # up the slot by 1\n",
    "                slot += 1\n",
    "                \n",
    "        print(f'\\t|{len(books[this_book])} slots logged')\n",
    "        \n",
    "print('done...')\n",
    "print(f'  {slot-1} slots created...')\n",
    "print(f'  {len(books)} books created...')\n",
    "print(f'  {len(chapters)} chapters created...')\n",
    "print(f'  {len(verses)} verses created...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction for Slot 99039\n",
    "\n",
    "Slot 99039 contains a very long word: \"SUGKATAKLHRONOMHQH/SONTAI\". This word is so long that there is no space between it and the type tag. The `split()` above thus mistakenly combines the surface form of the word with the type code. Based on a search of word length, this appears to be the only word with this problem. We will consider this a problem of conversion rather than a problem with the data source itself. Thus, instead of introducing an edit in the enrichments notebook, we make the change here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodes['trans'][99039] = nodes['trans'][99039][:25] # cut off the tag\n",
    "nodes['morph'][99039] = 'VC.' + nodes['morph'][99039]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUGKATAKLHRONOMHQH/SONTAI\n",
      "VC.APS2S.KLHRONOME/W.SUN.KATA\n"
     ]
    }
   ],
   "source": [
    "print(nodes['trans'][99039])\n",
    "print(nodes['morph'][99039])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count and Map Nodes\n",
    "\n",
    "Objects of slots are mapped to the oslot feature.\n",
    "\n",
    "The oslot feature is an edge feature with node as key and list/set as value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31862 nodes created...\n"
     ]
    }
   ],
   "source": [
    "edges = {'oslots':{}}\n",
    "\n",
    "# order book, chapter, verse by minimum slot number\n",
    "ordered_book =  [book[1] for book in sorted((books[bk][0], bk) for bk in books)]\n",
    "ordered_chap = [chap[1] for chap in sorted((chapters[ch][0], ch) for ch in chapters)]\n",
    "ordered_vers = [vers[1] for vers in sorted((verses[vs][0], vs) for vs in verses)]\n",
    "\n",
    "node = slot - 1 # slot is upped below\n",
    "\n",
    "# create book data\n",
    "for book in ordered_book:\n",
    "    node += 1\n",
    "    nodes['otype'][node] = 'book'\n",
    "    nodes['book'][node] = book\n",
    "    nodes['book@en'][node] = book\n",
    "    edges['oslots'][node] = books[book]\n",
    "\n",
    "# create chapter data\n",
    "for book, chapter in ordered_chap:\n",
    "    node += 1\n",
    "    nodes['otype'][node] = 'chapter'\n",
    "    nodes['chapter'][node] = str(chapter)\n",
    "    edges['oslots'][node] = chapters[(book, chapter)]\n",
    "    \n",
    "# create verse data\n",
    "for book, chapter, verse in ordered_vers:\n",
    "    node += 1\n",
    "    nodes['otype'][node] = 'verse'\n",
    "    nodes['verse'][node] = str(verse)\n",
    "    edges['oslots'][node] = verses[(book, chapter, verse)]\n",
    "\n",
    "    \n",
    "print(node-(slot-1), 'nodes created...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Metadata and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \n",
    "    '': {            \n",
    "            'created_by':'R. Kraft et al., CCAT, University of Pennsylvania',\n",
    "            'converted_by':'Cody Kingham',\n",
    "            'source':'http://ccat.sas.upenn.edu/rak//catss.html',\n",
    "            'license':'http://ccat.sas.upenn.edu/gopher/text/religion/biblical/lxxmorph/0-user-declaration.txt',\n",
    "        }\n",
    "            ,\n",
    "    'otext': {\n",
    "        'sectionFeatures': 'book,chapter,verse',\n",
    "        'sectionTypes': 'book,chapter,verse',\n",
    "        'fmt:text-orig-plain': '{trans}{trailer}',\n",
    "    },\n",
    "    'book@en': {\n",
    "        'valueType': 'str',\n",
    "        'language': 'English',\n",
    "        'languageCode': 'en',\n",
    "        'languageEnglish': 'english',\n",
    "    },\n",
    "}\n",
    "\n",
    "# add feature metadata\n",
    "integers = {}\n",
    "\n",
    "for nf in nodes:\n",
    "    metadata[nf] = {'valueType': 'str' if nf not in integers else 'int'}\n",
    "for ef in edges:\n",
    "    metadata[ef] = {'valueType': 'str' if ef not in integers else 'int'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.1.1\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "0 features found and 0 ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s Grid feature \"otype\" not found in\n",
      "\n",
      "  0.01s Grid feature \"oslots\" not found in\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.01s Grid feature \"otext\" not found. Working without Text-API\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bring in TF engine for export\n",
    "TF = Fabric(tf_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Exporting 8 node and 1 edge and 1 config features to tf:\n",
      "   |     0.00s T book                 to tf\n",
      "   |     0.00s T book@en              to tf\n",
      "   |     0.01s T chapter              to tf\n",
      "   |     0.87s T morph                to tf\n",
      "   |     0.20s T otype                to tf\n",
      "   |     0.73s T trailer              to tf\n",
      "   |     0.80s T trans                to tf\n",
      "   |     0.05s T verse                to tf\n",
      "   |     0.27s T oslots               to tf\n",
      "   |     0.00s M otext                to tf\n",
      "  2.94s Exported 8 node features and 1 edge features and 1 config features to tf\n"
     ]
    }
   ],
   "source": [
    "TF.save(nodeFeatures=nodes, edgeFeatures=edges, metaData=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Resulting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.1.1\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "10 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(locations=['tf'], modules=[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.22s T otype                from tf\n",
      "   |     1.22s T oslots               from tf\n",
      "   |     0.01s T book                 from tf\n",
      "   |     0.01s T chapter              from tf\n",
      "   |     0.10s T verse                from tf\n",
      "   |     1.45s T trailer              from tf\n",
      "   |     1.83s T trans                from tf\n",
      "   |      |     0.03s C __levels__           from otype, oslots\n",
      "   |      |     2.56s C __order__            from otype, oslots, __levels__\n",
      "   |      |     0.27s C __rank__             from otype, __order__\n",
      "   |      |     3.61s C __levUp__            from otype, oslots, __rank__\n",
      "   |      |     0.08s C __levDown__          from otype, __levUp__, __rank__\n",
      "   |      |     1.12s C __boundary__         from otype, oslots, __rank__\n",
      "   |     0.00s M otext                from tf\n",
      "   |      |     0.11s C __sections__         from otype, oslots, otext, __levUp__, __levels__, book, chapter, verse\n",
      "   |     1.65s T morph                from tf\n",
      "   |     0.00s T book@en              from tf\n",
      "   |     0.00s Feature overview: 8 for nodes; 1 for edges; 1 configs; 7 computed\n",
      "    14s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "api = TF.load('''\n",
    "                book chapter verse\n",
    "                trans morph\n",
    "              '''\n",
    "             )\n",
    "\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the Books that Are There"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genesis\n",
      "Exodus\n",
      "Leviticus\n",
      "Numbers\n",
      "Deuteronomy\n",
      "Joshua_B\n",
      "Joshua_A\n",
      "Judges_B\n",
      "Judges_A\n",
      "Ruth\n",
      "1_Samuel\n",
      "2_Samuel\n",
      "1_Kings\n",
      "2_Kings\n",
      "1_Chronicles\n",
      "2_Chronicles\n",
      "1_Esdras\n",
      "2_Esdras\n",
      "Esther\n",
      "Judith\n",
      "Tobit_BA\n",
      "Tobit_S\n",
      "1_Maccabees\n",
      "2_Maccabees\n",
      "3_Maccabees\n",
      "4_Maccabees\n",
      "Psalms\n",
      "Odes\n",
      "Proverbs\n",
      "Qoheleth\n",
      "Canticles\n",
      "Job\n",
      "Wisdom\n",
      "Sirach\n",
      "Psalms_of_Solomon\n",
      "Hosea\n",
      "Micah\n",
      "Amos\n",
      "Joel\n",
      "Jonah\n",
      "Obadiah\n",
      "Nahum\n",
      "Habakkuk\n",
      "Zephaniah\n",
      "Haggai\n",
      "Zechariah\n",
      "Malachi\n",
      "Isaiah\n",
      "Jeremiah\n",
      "Baruch\n",
      "Epistle_of_Jeremiah\n",
      "Lamentations\n",
      "Ezekiel\n",
      "Bel_and_Dragon_OG\n",
      "Bel_and_Dragon_Th\n",
      "Daniel_OG\n",
      "Daniel_Th\n",
      "Susanna_OG\n",
      "Susanna_Th\n"
     ]
    }
   ],
   "source": [
    "for book in F.otype.s('book'):\n",
    "    \n",
    "    print(F.book.v(book))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the Plain Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      "E)N A)RXH=| E)POI/HSEN O( QEO\\S TO\\N OU)RANO\\N KAI\\ TH\\N GH=N \n",
      "\n",
      "2\n",
      "H( DE\\ GH= H)=N A)O/RATOS KAI\\ A)KATASKEU/ASTOS KAI\\ SKO/TOS E)PA/NW TH=S A)BU/SSOU KAI\\ PNEU=MA QEOU= E)PEFE/RETO E)PA/NW TOU= U(/DATOS \n",
      "\n",
      "3\n",
      "KAI\\ EI)=PEN O( QEO/S GENHQH/TW FW=S KAI\\ E)GE/NETO FW=S \n",
      "\n",
      "4\n",
      "KAI\\ EI)=DEN O( QEO\\S TO\\ FW=S O(/TI KALO/N KAI\\ DIEXW/RISEN O( QEO\\S A)NA\\ ME/SON TOU= FWTO\\S KAI\\ A)NA\\ ME/SON TOU= SKO/TOUS \n",
      "\n",
      "5\n",
      "KAI\\ E)KA/LESEN O( QEO\\S TO\\ FW=S H(ME/RAN KAI\\ TO\\ SKO/TOS E)KA/LESEN NU/KTA KAI\\ E)GE/NETO E(SPE/RA KAI\\ E)GE/NETO PRWI/ H(ME/RA MI/A \n",
      "\n",
      "6\n",
      "KAI\\ EI)=PEN O( QEO/S GENHQH/TW STERE/WMA E)N ME/SW| TOU= U(/DATOS KAI\\ E)/STW DIAXWRI/ZON A)NA\\ ME/SON U(/DATOS KAI\\ U(/DATOS KAI\\ E)GE/NETO OU(/TWS \n",
      "\n",
      "7\n",
      "KAI\\ E)POI/HSEN O( QEO\\S TO\\ STERE/WMA KAI\\ DIEXW/RISEN O( QEO\\S A)NA\\ ME/SON TOU= U(/DATOS O(\\ H)=N U(POKA/TW TOU= STEREW/MATOS KAI\\ A)NA\\ ME/SON TOU= U(/DATOS TOU= E)PA/NW TOU= STEREW/MATOS \n",
      "\n",
      "8\n",
      "KAI\\ E)KA/LESEN O( QEO\\S TO\\ STERE/WMA OU)RANO/N KAI\\ EI)=DEN O( QEO\\S O(/TI KALO/N KAI\\ E)GE/NETO E(SPE/RA KAI\\ E)GE/NETO PRWI/ H(ME/RA DEUTE/RA \n",
      "\n",
      "9\n",
      "KAI\\ EI)=PEN O( QEO/S SUNAXQH/TW TO\\ U(/DWR TO\\ U(POKA/TW TOU= OU)RANOU= EI)S SUNAGWGH\\N MI/AN KAI\\ O)FQH/TW H( CHRA/ KAI\\ E)GE/NETO OU(/TWS KAI\\ SUNH/XQH TO\\ U(/DWR TO\\ U(POKA/TW TOU= OU)RANOU= EI)S TA\\S SUNAGWGA\\S AU)TW=N KAI\\ W)/FQH H( CHRA/ \n",
      "\n",
      "10\n",
      "KAI\\ E)KA/LESEN O( QEO\\S TH\\N CHRA\\N GH=N KAI\\ TA\\ SUSTH/MATA TW=N U(DA/TWN E)KA/LESEN QALA/SSAS KAI\\ EI)=DEN O( QEO\\S O(/TI KALO/N \n",
      "\n",
      "11\n",
      "KAI\\ EI)=PEN O( QEO/S BLASTHSA/TW H( GH= BOTA/NHN XO/RTOU SPEI=RON SPE/RMA KATA\\ GE/NOS KAI\\ KAQ' O(MOIO/THTA KAI\\ CU/LON KA/RPIMON POIOU=N KARPO/N OU(= TO\\ SPE/RMA AU)TOU= E)N AU)TW=| KATA\\ GE/NOS E)PI\\ TH=S GH=S KAI\\ E)GE/NETO OU(/TWS \n",
      "\n",
      "12\n",
      "KAI\\ E)CH/NEGKEN H( GH= BOTA/NHN XO/RTOU SPEI=RON SPE/RMA KATA\\ GE/NOS KAI\\ KAQ' O(MOIO/THTA KAI\\ CU/LON KA/RPIMON POIOU=N KARPO/N OU(= TO\\ SPE/RMA AU)TOU= E)N AU)TW=| KATA\\ GE/NOS E)PI\\ TH=S GH=S KAI\\ EI)=DEN O( QEO\\S O(/TI KALO/N \n",
      "\n",
      "13\n",
      "KAI\\ E)GE/NETO E(SPE/RA KAI\\ E)GE/NETO PRWI/ H(ME/RA TRI/TH \n",
      "\n",
      "14\n",
      "KAI\\ EI)=PEN O( QEO/S GENHQH/TWSAN FWSTH=RES E)N TW=| STEREW/MATI TOU= OU)RANOU= EI)S FAU=SIN TH=S GH=S TOU= DIAXWRI/ZEIN A)NA\\ ME/SON TH=S H(ME/RAS KAI\\ A)NA\\ ME/SON TH=S NUKTO\\S KAI\\ E)/STWSAN EI)S SHMEI=A KAI\\ EI)S KAIROU\\S KAI\\ EI)S H(ME/RAS KAI\\ EI)S E)NIAUTOU\\S \n",
      "\n",
      "15\n",
      "KAI\\ E)/STWSAN EI)S FAU=SIN E)N TW=| STEREW/MATI TOU= OU)RANOU= W(/STE FAI/NEIN E)PI\\ TH=S GH=S KAI\\ E)GE/NETO OU(/TWS \n",
      "\n",
      "16\n",
      "KAI\\ E)POI/HSEN O( QEO\\S TOU\\S DU/O FWSTH=RAS TOU\\S MEGA/LOUS TO\\N FWSTH=RA TO\\N ME/GAN EI)S A)RXA\\S TH=S H(ME/RAS KAI\\ TO\\N FWSTH=RA TO\\N E)LA/SSW EI)S A)RXA\\S TH=S NUKTO/S KAI\\ TOU\\S A)STE/RAS \n",
      "\n",
      "17\n",
      "KAI\\ E)/QETO AU)TOU\\S O( QEO\\S E)N TW=| STEREW/MATI TOU= OU)RANOU= W(/STE FAI/NEIN E)PI\\ TH=S GH=S \n",
      "\n",
      "18\n",
      "KAI\\ A)/RXEIN TH=S H(ME/RAS KAI\\ TH=S NUKTO\\S KAI\\ DIAXWRI/ZEIN A)NA\\ ME/SON TOU= FWTO\\S KAI\\ A)NA\\ ME/SON TOU= SKO/TOUS KAI\\ EI)=DEN O( QEO\\S O(/TI KALO/N \n",
      "\n",
      "19\n",
      "KAI\\ E)GE/NETO E(SPE/RA KAI\\ E)GE/NETO PRWI/ H(ME/RA TETA/RTH \n",
      "\n",
      "20\n",
      "KAI\\ EI)=PEN O( QEO/S E)CAGAGE/TW TA\\ U(/DATA E(RPETA\\ YUXW=N ZWSW=N KAI\\ PETEINA\\ PETO/MENA E)PI\\ TH=S GH=S KATA\\ TO\\ STERE/WMA TOU= OU)RANOU= KAI\\ E)GE/NETO OU(/TWS \n",
      "\n",
      "21\n",
      "KAI\\ E)POI/HSEN O( QEO\\S TA\\ KH/TH TA\\ MEGA/LA KAI\\ PA=SAN YUXH\\N ZW/|WN E(RPETW=N A(\\ E)CH/GAGEN TA\\ U(/DATA KATA\\ GE/NH AU)TW=N KAI\\ PA=N PETEINO\\N PTERWTO\\N KATA\\ GE/NOS KAI\\ EI)=DEN O( QEO\\S O(/TI KALA/ \n",
      "\n",
      "22\n",
      "KAI\\ HU)LO/GHSEN AU)TA\\ O( QEO\\S LE/GWN AU)CA/NESQE KAI\\ PLHQU/NESQE KAI\\ PLHRW/SATE TA\\ U(/DATA E)N TAI=S QALA/SSAIS KAI\\ TA\\ PETEINA\\ PLHQUNE/SQWSAN E)PI\\ TH=S GH=S \n",
      "\n",
      "23\n",
      "KAI\\ E)GE/NETO E(SPE/RA KAI\\ E)GE/NETO PRWI/ H(ME/RA PE/MPTH \n",
      "\n",
      "24\n",
      "KAI\\ EI)=PEN O( QEO/S E)CAGAGE/TW H( GH= YUXH\\N ZW=SAN KATA\\ GE/NOS TETRA/PODA KAI\\ E(RPETA\\ KAI\\ QHRI/A TH=S GH=S KATA\\ GE/NOS KAI\\ E)GE/NETO OU(/TWS \n",
      "\n",
      "25\n",
      "KAI\\ E)POI/HSEN O( QEO\\S TA\\ QHRI/A TH=S GH=S KATA\\ GE/NOS KAI\\ TA\\ KTH/NH KATA\\ GE/NOS KAI\\ PA/NTA TA\\ E(RPETA\\ TH=S GH=S KATA\\ GE/NOS AU)TW=N KAI\\ EI)=DEN O( QEO\\S O(/TI KALA/ \n",
      "\n",
      "26\n",
      "KAI\\ EI)=PEN O( QEO/S POIH/SWMEN A)/NQRWPON KAT' EI)KO/NA H(METE/RAN KAI\\ KAQ' O(MOI/WSIN KAI\\ A)RXE/TWSAN TW=N I)XQU/WN TH=S QALA/SSHS KAI\\ TW=N PETEINW=N TOU= OU)RANOU= KAI\\ TW=N KTHNW=N KAI\\ PA/SHS TH=S GH=S KAI\\ PA/NTWN TW=N E(RPETW=N TW=N E(RPO/NTWN E)PI\\ TH=S GH=S \n",
      "\n",
      "27\n",
      "KAI\\ E)POI/HSEN O( QEO\\S TO\\N A)/NQRWPON KAT' EI)KO/NA QEOU= E)POI/HSEN AU)TO/N A)/RSEN KAI\\ QH=LU E)POI/HSEN AU)TOU/S \n",
      "\n",
      "28\n",
      "KAI\\ HU)LO/GHSEN AU)TOU\\S O( QEO\\S LE/GWN AU)CA/NESQE KAI\\ PLHQU/NESQE KAI\\ PLHRW/SATE TH\\N GH=N KAI\\ KATAKURIEU/SATE AU)TH=S KAI\\ A)/RXETE TW=N I)XQU/WN TH=S QALA/SSHS KAI\\ TW=N PETEINW=N TOU= OU)RANOU= KAI\\ PA/NTWN TW=N KTHNW=N KAI\\ PA/SHS TH=S GH=S KAI\\ PA/NTWN TW=N E(RPETW=N TW=N E(RPO/NTWN E)PI\\ TH=S GH=S \n",
      "\n",
      "29\n",
      "KAI\\ EI)=PEN O( QEO/S I)DOU\\ DE/DWKA U(MI=N PA=N XO/RTON SPO/RIMON SPEI=RON SPE/RMA O(/ E)STIN E)PA/NW PA/SHS TH=S GH=S KAI\\ PA=N CU/LON O(\\ E)/XEI E)N E(AUTW=| KARPO\\N SPE/RMATOS SPORI/MOU U(MI=N E)/STAI EI)S BRW=SIN \n",
      "\n",
      "30\n",
      "KAI\\ PA=SI TOI=S QHRI/OIS TH=S GH=S KAI\\ PA=SI TOI=S PETEINOI=S TOU= OU)RANOU= KAI\\ PANTI\\ E(RPETW=| TW=| E(/RPONTI E)PI\\ TH=S GH=S O(\\ E)/XEI E)N E(AUTW=| YUXH\\N ZWH=S PA/NTA XO/RTON XLWRO\\N EI)S BRW=SIN KAI\\ E)GE/NETO OU(/TWS \n",
      "\n",
      "31\n",
      "KAI\\ EI)=DEN O( QEO\\S TA\\ PA/NTA O(/SA E)POI/HSEN KAI\\ I)DOU\\ KALA\\ LI/AN KAI\\ E)GE/NETO E(SPE/RA KAI\\ E)GE/NETO PRWI/ H(ME/RA E(/KTH \n"
     ]
    }
   ],
   "source": [
    "gen_1 = T.nodeFromSection(('Genesis', '1'))\n",
    "\n",
    "for verse in L.d(gen_1, otype='verse'):\n",
    "    \n",
    "    words = L.d(verse, otype='word')\n",
    "    \n",
    "    print()\n",
    "    print(F.verse.v(verse))\n",
    "    print(T.text(words, fmt='text-orig-plain'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the Morphology Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAKA/RIOS  —  A1A.NSM.MAKA/RIOS\n",
      "A)NH/R  —  N3.NSM.A)NH/R\n",
      "O(\\S  —  RR.NSM.O(/S\n",
      "OU)K  —  D.OU)\n",
      "E)POREU/QH  —  VCI.API3S.POREU/OMAI\n",
      "E)N  —  P.E)N\n",
      "BOULH=|  —  N1.DSF.BOULH/\n",
      "A)SEBW=N  —  A3H.GPM.A)SEBH/S\n",
      "KAI\\  —  C.KAI/\n",
      "E)N  —  P.E)N\n",
      "O(DW=|  —  N2.DSF.O(DO/S\n",
      "A(MARTWLW=N  —  A1B.GPM.A(MARTWLO/S\n",
      "OU)K  —  D.OU)\n",
      "E)/STH  —  VHI.AAI3S.I(/STHMI\n",
      "KAI\\  —  C.KAI/\n",
      "E)PI\\  —  P.E)PI/\n",
      "KAQE/DRAN  —  N1A.ASF.KAQE/DRA\n",
      "LOIMW=N  —  N2.GPM.LOIMO/S\n",
      "OU)K  —  D.OU)\n",
      "E)KA/QISEN  —  VAI.AAI3S.I(/ZW.KATA\n"
     ]
    }
   ],
   "source": [
    "for word in L.d(T.nodeFromSection(('Psalms', '1', '1')), otype='word'):\n",
    "    \n",
    "    print(F.trans.v(word), ' — ', F.morph.v(word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
