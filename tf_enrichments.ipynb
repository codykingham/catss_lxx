{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATSS Text-Fabric Enrichments\n",
    "\n",
    "The raw data from the CCAT resource has been converted into a TF format in `tf_conversion.ipynb`. This notebook enriches the dataset in a number of ways:\n",
    "\n",
    "* Morphological tags are split and parsed into individual word-level features.\n",
    "* Word-level plain-text is processed into a UTF8 representation feature.\n",
    "* A first effort is made to connect the ETCBC BHSA Hebrew database with the CATSS Hebrew parallel text using parts of speech for word-level connections. Phrase-level connections are created based on phrases' presence per line.\n",
    "* Some rudimentary phrase divisions are also added the CATSS data based on the parallel data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections, glob, re, Levenshtein\n",
    "from tf.fabric import Fabric\n",
    "from greekutils import beta2unicode # do: pip install greek-utils==0.2\n",
    "\n",
    "# some cells are blocked from being run/\n",
    "# toggle True to allow a given task\n",
    "run_corrections = False\n",
    "run_morphology = False\n",
    "run_unicode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrections\n",
    "\n",
    "Corrections to the data based on the decoding processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_corrections toggled False. Doing nothing...\n"
     ]
    }
   ],
   "source": [
    "metadata = metadata = {\n",
    "\n",
    "'': {            \n",
    "        'created_by':'R. Kraft et al., CCAT, University of Pennsylvania',\n",
    "        'converted_by':'Cody Kingham',\n",
    "        'source':'http://ccat.sas.upenn.edu/rak//catss.html',\n",
    "        'license':'http://ccat.sas.upenn.edu/gopher/text/religion/biblical/lxxmorph/0-user-declaration.txt',\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# TO FIX, 2017-12-11:\n",
    "# advb - 173450 (D.P.E)PA/NW) 1 == styp == D.P\n",
    "# 5058 & 5159 (N.N.M.*MESRAIM) 0-2 == styp == N.N.M.\n",
    "\n",
    "run_corrections = False\n",
    "\n",
    "if run_corrections:\n",
    "\n",
    "    # instantiate/load old data\n",
    "    TF = Fabric(locations=['tf'], modules=[''])\n",
    "    api = TF.load('book chapter verse morph trans', silent=True)\n",
    "    api.makeAvailableIn(globals())\n",
    "    \n",
    "    nodes = collections.defaultdict(dict)\n",
    "\n",
    "    # prime metadata\n",
    "    metadata['morph'] = {'valueType': 'str'}\n",
    "    \n",
    "    # corrections for morph feature\n",
    "    for word in F.otype.s('word'):\n",
    "\n",
    "        if word == 11436:\n",
    "            new_morph = 'VA.AAD2P.I(/ZW.KATA'\n",
    "            nodes['morph'][word] = new_morph\n",
    "        else:\n",
    "            nodes['morph'][word] = F.morph.v(word)\n",
    "\n",
    "    save_TF = Fabric(locations=['tf'], modules=['enrichments'])\n",
    "    save_TF.save(nodeFeatures=nodes, metaData=metadata)\n",
    "    del metadata['morph'] # clean out metadata dict\n",
    "    \n",
    "    print('corrections done!')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('run_corrections toggled False. Doing nothing...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Enriched, Corrected CATSS Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.1.1\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "22 features found and 1 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.00s B book                 from tf\n",
      "   |     0.00s B chapter              from tf\n",
      "   |     0.01s B verse                from tf\n",
      "   |     0.17s B trans                from tf\n",
      "   |     0.20s B morph                from tf/enrichments\n",
      "   |     0.15s B typ                  from tf/enrichments\n",
      "   |     0.00s Feature overview: 20 for nodes; 1 for edges; 1 configs; 7 computed\n",
      "  1.02s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "# load in enriched data\n",
    "TF = Fabric(locations=['tf'], modules=['', 'enrichments'])\n",
    "api = TF.load('book chapter verse morph trans typ')\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate New Morphology Features\n",
    "\n",
    "See the [CATSS morphology documentation](http://ccat.sas.upenn.edu/gopher/text/religion/biblical/lxxmorph/*Morph-Coding). In the source data, morphology is space-separated. In the TF version they are dot separated. Tags have to be split, recognized, and converted. They are added as separate word-level features.\n",
    "\n",
    "Morphology codes have 3 or 2 columns, depending on part of speech type. From the documentation:\n",
    "\n",
    "> 1. \"TYPE\" CODES (3 columns maximum, to identify part of speech)\n",
    "> 2. \"PARSE\" CODE (up to 6 columns, as needed, to parse each form) [\\*OPTIONAL]\n",
    "> 3. [lexeme]\n",
    "\n",
    "Proposals for new features:\n",
    "\n",
    "* typ — part of speech, derived from the type codes. It is the first letter of the type code and can have a values of (bold is proposed new feature name):\n",
    "    * N — noun — **noun**\n",
    "    * A — adjective — **adjv**\n",
    "    * R — pronoun — **pron**\n",
    "    * C — conjunction — **conj**\n",
    "    * X — particle — **part**\n",
    "    * I — interjection — **intj**\n",
    "    * M — indeclinable number — **inum**\n",
    "    * P — preposition — **prep**\n",
    "    * D — adverb — **advb**\n",
    "* **styp** — subtype of part of speech, e.g. 1st declension, 3rd declension of various stems. There are lots of categories. The simple code is preserved. Refer to the documentation for their meanings. For that code, I preserve also the part of speech value (N, A, R, etc.)\n",
    "\n",
    "* case — **case**\n",
    "* gender — **gender**\n",
    "* number — **number**\n",
    "* tense — **tense**\n",
    "* voice — **voice**\n",
    "* mood — **mood**\n",
    "* person — **person**\n",
    "* degree — **degree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_morphology toggled False. Doing nothing...\n"
     ]
    }
   ],
   "source": [
    "# Notes to myself\n",
    "# prototypical counts per type:\n",
    "# 3 - adjv, noun, verb\n",
    "# 2 - advb, conj, intj, part, prep, inum\n",
    "# 3 - inum, pron, propn\n",
    "# 4 - propn (N.N.M.MESRAIM), verb (participle)\n",
    "\n",
    "# those with overloaded lexemes:\n",
    "# verb(>3, not participle), verb(>4, participle)\n",
    "\n",
    "# store new features here: feature_name to node to feature \n",
    "features = collections.defaultdict(dict)\n",
    "\n",
    "# conversion dicts\n",
    "typs = {'N': 'noun',\n",
    "        'V': 'verb',\n",
    "        'A': 'adjv',\n",
    "        'R': 'pron',\n",
    "        'C': 'conj',\n",
    "        'X': 'part',\n",
    "        'I': 'intj',\n",
    "        'M': 'inum',\n",
    "        'P': 'prep',\n",
    "        'D': 'advb'}\n",
    "       #'N': 'propn' proper noun, added below with special rule\n",
    "    \n",
    "# nominals \n",
    "# [case][number][gender]\n",
    "cases = {'N': 'nom',\n",
    "         'G': 'gen',\n",
    "         'D': 'dat',\n",
    "         'A': 'acc',\n",
    "         'V': 'voc'}\n",
    "numbers = {'S': 'sg',\n",
    "          'D': 'du',\n",
    "          'P': 'pl'}\n",
    "genders = {'M': 'm',\n",
    "          'F': 'f',\n",
    "          'N': 'n'}\n",
    "degrees = {'C': 'comparative',\n",
    "          'S': 'superlative'}\n",
    "\n",
    "# verbs\n",
    "# [tense][voice][mood][person][number] [case][number][gender]\n",
    "\n",
    "tenses = {'P': 'present',\n",
    "         'I': 'imperfect',\n",
    "         'F': 'future',\n",
    "         'A': 'aorist',\n",
    "         'X': 'perfect',\n",
    "         'Y': 'pluperfect'}\n",
    "voices = {'A': 'active',\n",
    "         'M': 'middle',\n",
    "         'P': 'passsive'}\n",
    "moods = {'I': 'indc',\n",
    "         'D': 'impv',\n",
    "         'S': 'subj',\n",
    "         'O': 'optv',\n",
    "         'N': 'infv',\n",
    "         'P': 'ptcp'}\n",
    "\n",
    "if run_morphology:\n",
    "\n",
    "    # big loop\n",
    "    for word in F.otype.s('word'):\n",
    "\n",
    "        morph = F.morph.v(word)\n",
    "        split_morph = morph.split('.')\n",
    "\n",
    "        # parse morphology codes in order of appearance:\n",
    "\n",
    "\n",
    "        # 1. assign subtypes and types\n",
    "\n",
    "        styp = split_morph[0] # subtype\n",
    "\n",
    "        # get type; exception for proper nouns; nouns with no subtypes\n",
    "        if styp == 'N':\n",
    "            typ = 'propn'\n",
    "        else:\n",
    "            typ = typs[styp[0]] # type is only first char of code, convert it\n",
    "\n",
    "\n",
    "        # 2. assign parsing data\n",
    "\n",
    "        # indeclinable words\n",
    "        if len(split_morph) == 2 or typ in {'advb', 'conj'}:\n",
    "            case, gender, number, degree, tense, voice, mood, person = ('' for i in range(1,9))\n",
    "            lexeme = '.'.join(split_morph[1:])\n",
    "\n",
    "\n",
    "        # nominal words with case/gender/number\n",
    "        elif typ in {'adjv', 'noun', 'inum', 'pron', 'propn'}:\n",
    "\n",
    "            parsing_data = split_morph[1]\n",
    "            case = ''\n",
    "            gender = ''\n",
    "            number = ''\n",
    "            degree = ''\n",
    "\n",
    "            # get parsing; some parsing codes have < 3 values, loop is thus necessary\n",
    "            for i, char in enumerate(parsing_data):\n",
    "\n",
    "                # dative/dual disambiguation\n",
    "                if i == 0 and char == 'D': \n",
    "                    case = 'dat'\n",
    "                elif i != 0 and char == 'D':\n",
    "                    number = 'du'\n",
    "\n",
    "                # disambiguation for 'S' superlative\n",
    "                elif all([char == 'S' or char == 'C', len(parsing_data) == 4,\n",
    "                          typ == 'adjv', i != 1]):\n",
    "                    degree = degrees.get(char, '')\n",
    "\n",
    "                # all other parsings\n",
    "                elif char != 'D':\n",
    "                    case = cases.get(char, '') if not case else case\n",
    "                    gender = genders.get(char, '') if not gender else gender\n",
    "                    number = numbers.get(char, '') if not number else number\n",
    "                    degree = '' if not degree else degree\n",
    "\n",
    "            # set non applicable values to null\n",
    "            person, tense, voice, mood = ('' for i in range(1,5))\n",
    "\n",
    "            lexeme = '.'.join(split_morph[2:])\n",
    "\n",
    "\n",
    "        # verbs\n",
    "        elif typ == 'verb':\n",
    "\n",
    "            parsing_data = split_morph[1]\n",
    "            tense = tenses[parsing_data[0] ]\n",
    "            try:\n",
    "                voice = voices[parsing_data[1]]\n",
    "\n",
    "            except:\n",
    "                print(word, morph)\n",
    "            mood = moods[parsing_data[2]]\n",
    "\n",
    "            # handle participles \n",
    "            try:\n",
    "                gender = genders[parsing_data[5]] # only participles have >4 chars\n",
    "                number = numbers[parsing_data[4]]\n",
    "                case = cases[parsing_data[3]]\n",
    "                person = '' # non-applicable values\n",
    "                degree = ''\n",
    "\n",
    "            except IndexError:\n",
    "\n",
    "                # all normal verbs\n",
    "                try:\n",
    "                    person = parsing_data[3]\n",
    "                    number = numbers[parsing_data[4]]\n",
    "                    case = '' # non-applicable values\n",
    "                    gender = '' \n",
    "                    degree = ''\n",
    "\n",
    "                # handle infinitives\n",
    "                except IndexError: \n",
    "                    person = '' # non-applicable values\n",
    "                    number = ''\n",
    "                    case = ''\n",
    "                    gender = ''\n",
    "                    degree = ''\n",
    "\n",
    "            lexeme = '.'.join(split_morph[2:])\n",
    "\n",
    "        # assign features\n",
    "        features['typ'][word] = typ\n",
    "        features['styp'][word] = styp\n",
    "        features['lex'][word] = lexeme\n",
    "        features['case'][word] = case\n",
    "        features['number'][word] = number\n",
    "        features['gender'][word] = gender\n",
    "        features['degree'][word] = degree\n",
    "        features['tense'][word] = tense\n",
    "        features['voice'][word] = voice\n",
    "        features['mood'][word] = mood\n",
    "        features['person'][word] = person\n",
    "\n",
    "    print(f'done! with {len(features)} new features logged...')\n",
    "    \n",
    "    # EXPORT new features\n",
    "    \n",
    "    for feat in features:\n",
    "        metadata[feat] = {'valueType': 'str'}\n",
    "    \n",
    "    TF.save(nodeFeatures=features, metaData=metadata)\n",
    "    \n",
    "else:\n",
    "    print('run_morphology toggled False. Doing nothing...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate UTF8 Feature\n",
    "\n",
    "The transcription table is available at [the CCAT documentation page](http://ccat.sas.upenn.edu/gopher/text/religion/biblical/lxxmorph/0-betacode.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_unicode toggled False. Doing nothing...\n"
     ]
    }
   ],
   "source": [
    "if run_unicode:\n",
    "    \n",
    "    for word in F.otype.s('word'):\n",
    "\n",
    "        unicode = beta2unicode.convert(F.trans.v(word))\n",
    "\n",
    "        # add final sigma fix!\n",
    "        if unicode[-1] == 'σ':\n",
    "            unicode = unicode[:-1] + 'ς' \n",
    "            \n",
    "        # save feature\n",
    "        features['utf8'][word] = unicode\n",
    "            \n",
    "    # export unicode features\n",
    "    for feat in features:\n",
    "        metadata[feat] = {'valueType': 'str'}\n",
    "    \n",
    "    TF.save(nodeFeatures=features, metaData=metadata)\n",
    "    \n",
    "else: \n",
    "    print('run_unicode toggled False. Doing nothing...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Integration of Parallel Data with ETCBC Hebrew Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.1.1\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "114 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.11s B g_cons               from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s Feature overview: 108 for nodes; 5 for edges; 1 configs; 7 computed\n",
      "  3.67s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "# instantiate ETCBC data\n",
    "\n",
    "hebrew_etcbc = Fabric(locations='~/github/etcbc/bhsa/tf', modules='c')\n",
    "etcbc = hebrew_etcbc.load('g_cons')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method\n",
    "\n",
    "The connections are made with two processes.\n",
    "\n",
    "The process 1 involves matching the data in the CATSS parallel files to their respective databases. After the Hebrew/Greek data is sorted out from the text-critical notations, surface forms are matched (within 1 character, using Levenshtein distance) to the corresponding transcription in BHSA/CATSS TF data. Once matched, the relevant slot data is dropped into a tuple which is in turn keyed to its line number in the dictionaries `greekLine_slots` and `hebrewLine_slots` (to be accessed in process 2).\n",
    "\n",
    "The process 2 involves matching the Hebrew and Greek language data using the tuples stored in process 1. Those matches are made based on matching part of speech tags. The matches are made in sequence to avoid mismatching multiple part of speech tags. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCAT to ETCBC Transcription\n",
    "ccat_etcbc = {')': '>',\n",
    "             'B': 'B',\n",
    "             'G': 'G',\n",
    "             'D': 'D',\n",
    "             'H': 'H',\n",
    "             'W': 'W',\n",
    "             'Z': 'Z',\n",
    "             'X': 'X',\n",
    "             '+': 'V',\n",
    "             'Y': 'J',\n",
    "             'K': 'K',\n",
    "             'L': 'L',\n",
    "             'M': 'M',\n",
    "             'N': 'N',\n",
    "             'S': 'S',\n",
    "             '(': '<',\n",
    "             'P': 'P',\n",
    "             'C': 'Y',\n",
    "             'Q': 'Q',\n",
    "             'R': 'R',\n",
    "             '&': 'F',\n",
    "             '$': 'C',\n",
    "             'T': 'T',\n",
    "             }\n",
    "\n",
    "# characters for detecting Hebrew/Greek surface forms\n",
    "hchars = ''.join(ccat_etcbc.keys()) # string of hebrew characters in transcription\n",
    "gchars = ''.join(set(l for w in F.otype.s('word') # string of greek characters in transcription\n",
    "                       for l in F.trans.v(w)\n",
    "                    ))\n",
    "gchars = gchars.replace('\\\\', '\\\\\\\\').replace('-', '\\-') # fix escape bug with re module\n",
    "\n",
    "# regex to extract Hebrew/Greek words\n",
    "hwords = f'^\\*?[/{hchars}]+|\\s\\*?[/{hchars}]+'\n",
    "gwords = f'^[{gchars}]*|\\s[{gchars}]*'\n",
    "\n",
    "# TEXTUAL CRITICISM NOTATIONS:\n",
    "# regex patterns + descriptions\n",
    "# to find and add:\n",
    "# ~, ~~~\n",
    "            \n",
    "# tc chars + descriptions (no capture groups)\n",
    "tchars = {'{#}':\n",
    "              'Asterized passage (in Job).',\n",
    "          '{g}':\n",
    "              'Reference to difference between the text of Rahlfs and that of the relevant Gšttingen edition.',\n",
    "          f'\\*\\*[/?{hchars}]+':\n",
    "              'Qere',\n",
    "          '\\*\\*?z': \n",
    "              'Qere wela ketib, ketib wela qere.',\n",
    "          '\\.\\.a':\n",
    "              'Word included in one of the Aramaic sections.',\n",
    "          '\\[\\d+\\??\\]':\n",
    "              'Reference of number of verse in LXX, different from MT.',\n",
    "          '\\[\\[\\d+\\??\\]\\]':\n",
    "              'Reference number of verse in MT, different from the LXX.',\n",
    "          '---\\s{x}':\n",
    "              'Apparent minus',\n",
    "          '--\\+\\s{x}':\n",
    "              'Apparent plus created by lack of equivalence between long stretches of text in the LXX and MT.',\n",
    "          '---?\\s|---?\\t|---?$':\n",
    "              'In the Greek column:  Hebrew counterpart lacking in the LXX (minus in the LXX).',\n",
    "          '-?--\\+':\n",
    "              'In col a. of the Hebrew:  element added in the Greek (plus in the LXX).',\n",
    "          '\\'\\'|\\'':\n",
    "              'Long minus or plus (at least four lines).',\n",
    "          '{\\.\\.~}':# CHECK\n",
    "              'Stylistic or grammatical transposition.',\n",
    "          '{d\\??}':\n",
    "              'Reference to doublet (occurring between the two elements of the doublet.',\n",
    "          '{'+f'\\.\\.\\.[/?\\s,{hchars}]*'+'}|{'+f'\\.\\.\\.[{gchars}\\s/?]*' + '}':\n",
    "              'Equivalent reflected elsewhere in the text, disregarded by indexing program.',\n",
    "          '{'+f'\\.\\.d[{gchars}]*'+'}': \n",
    "              'Distributive rendering, occurring once in the translation but referring to more than one Hebrew word.',\n",
    "          '{'+f'\\.\\.\\^[\\s{gchars}]*'+'}': \n",
    "              '[unknown]',\n",
    "          '{'+ f'\\.\\.r[/?,{hchars}]*'+'}': \n",
    "              'Notation in Hebrew column of elements repeated in the translation.',\n",
    "          '\\?': \n",
    "              'Questionable notation, equivalent, etc.',\n",
    "          '{p}':\n",
    "              'Greek preverb representing Hebrew preposition.',\n",
    "          '{'+ f'\\.\\.p[{gchars}]*' +'}': \n",
    "              'Preposition added in the LXX in accordance with the rules of the Greek language or translational habits.',\n",
    "          '{!}[a-z\\-]*': \n",
    "              'Infinitive absolute',\n",
    "          '{s}':\n",
    "              'Hebrew M/, MN (comparative, superlative) reflected by Greek comparative or superlative.',\n",
    "          '{t}':\n",
    "              'Transliterated Hebrew word.',\n",
    "          '{v}':\n",
    "              ' The reading of the main text of the LXX seems to reflect a secondary text, while the original reading is reflected in a variant.',\n",
    "          '=%\\s|=%$':\n",
    "              'Introducing categories of translation technique recorded in col. b.',\n",
    "          '=%vap|-%vap': # second one is ft. a mistake in text\n",
    "              'Change from active to passive form in verbs.',\n",
    "          '=%vpa|%vpa': # second one mistake?\n",
    "              'Change from passive to active form in verbs.',\n",
    "          '=%p\\s|=%p$':\n",
    "              'Difference in preposition or particle.',\n",
    "          '=%p\\+|=%p-':\n",
    "              'Addition[/subtraction] of preposition or particle.',\n",
    "          '=p%-':\n",
    "              'Omission of preposition or particle.',\n",
    "          f'=;\\S*|;=\\S*': # second maybe mistake\n",
    "              'Retroversion in col. b based on equivalence occurring in immediate or remote context.',\n",
    "          '^G\\s|\\sG\\s|\\sG$':\n",
    "              'Hebrew variant, but at this stage no plausible retroversion is suggested.',\n",
    "          '=\\+|-\\+':\n",
    "              'Difference in numbers between MT and the LXX.',\n",
    "          '=?@\\S*':\n",
    "              'Etymological exegesis.',\n",
    "          f'=@[?/\\s,{hchars}]*a':\n",
    "              'Etymological exegesis according to Aramaic.',\n",
    "          f'=:\\S*':\n",
    "              'Introducing reconstructed proper noun.',\n",
    "          '=v\\s':\n",
    "              'Difference in vocalization (reading).',\n",
    "          '=vs':\n",
    "              'Difference in vocalization (reading). [shin/sin]',\n",
    "          f'=r[/?\\s,{hchars}]*':\n",
    "              'Incomplete retroversion.',\n",
    "          '{\\*}':\n",
    "              'Agreement of LXX with ketib.',\n",
    "          '{\\*\\*}':\n",
    "              'Agreement of LXX with qere.',\n",
    "          '^\\.\\s|\\s\\.\\s|\\s\\.$':\n",
    "              'Interchange of consonants between MT and the presumed Hebrew parent text of the LXX.',\n",
    "          '\\.rd\\S*':\n",
    "              'Interchange of consonants R/D, etc.',\n",
    "          '\\.m\\S*':\n",
    "              'Interchange of consonants, metathesis',\n",
    "          '\\.z\\S*':\n",
    "              'Possible abbreviation',\n",
    "          '\\.s\\S*':\n",
    "              'One word of MT separated into two or more words in the parent text of the LXX.',\n",
    "          '\\.j\\S*':\n",
    "              'Two words of MT joined into one word in the parent text of the LXX.',\n",
    "          '\\.w\\S*':\n",
    "              'Different word-division reflected in the parent text of the LXX.',\n",
    "          '<.*>':\n",
    "              '[verse backreference/editor remark?]',\n",
    "          '\\^+':\n",
    "              '[difference in sequence?]',\n",
    "          '\\.\\S*':\n",
    "              ['unknown'],\n",
    "            '{.*}': \n",
    "              '[catch other comments]',\n",
    "            '--=\\S*':\n",
    "              '[unknown]',\n",
    "           '<\\S*\\s|<\\S*\\t|<\\S*$':\n",
    "              '[incomplete notes?]',\n",
    "           ':.*':\n",
    "              '[unknown]',\n",
    "           '--\\S':\n",
    "              '[unknown]',\n",
    "           ',\\S*':\n",
    "              '[unknown]',\n",
    "           '{\\S*':\n",
    "              '[ft. broken angle bracket]'\n",
    "             }\n",
    "\n",
    "old_equal = f'=[?/\\s\\S,{hchars}'+'{}]+'\n",
    "\n",
    "col_b = {'=\\S*':\n",
    "              'Introducing col. b of the Hebrew (a selection of retroverted readings, presumably found in the parent text of the LXX).',}\n",
    "\n",
    "tcharsG = r'|'.join(tchars)\n",
    "tchars.update(col_b)\n",
    "tchars = r'|'.join(tchars)\n",
    "\n",
    "def convert_ccat(string):\n",
    "    \n",
    "    '''\n",
    "    Simply assembles and returns an ETCBC transcription string\n",
    "    from the CCAT Hebrew transcription.\n",
    "    '''\n",
    "    \n",
    "    converted = ''.join(ccat_etcbc[char] if char in ccat_etcbc else char\n",
    "                            for char in string)\n",
    "    \n",
    "    return converted\n",
    "    \n",
    "def is_match(hebrew_list, greek_list, tolerance=2):\n",
    "    \n",
    "    '''\n",
    "    Match two supplied lists of surface forms.\n",
    "    Returns boolean.\n",
    "    tolerance defines the number of characters that can differ per word.\n",
    "    '''\n",
    "    \n",
    "    # check word by word\n",
    "    for hebrew_w, greek_w in zip(hebrew_list, greek_list):\n",
    "        if Levenshtein.distance(hebrew_w, greek_w) > tolerance:\n",
    "            return False\n",
    "    # true if it reaches this point\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "source/parallel/11.1Sam.par – 6644\nSRC: \tA)NAQEMATIEI=S AU)TO\\N})T KL )$R\tKAI\\ PA/NTA TA\\ {d} {...KAI\\ PA/NTA TA\\}\nHEB:['A)NAQEMATIEI', 'AU)TO\\\\N})T', 'KL', ')$R']\nTC: =S",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-715833219701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc_hebrew\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{file} – {i+1}\\nSRC: {line}\\nHEB:{c_hebrew}\\nTC: {hebrew_tc}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mc_greek\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: source/parallel/11.1Sam.par – 6644\nSRC: \tA)NAQEMATIEI=S AU)TO\\N})T KL )$R\tKAI\\ PA/NTA TA\\ {d} {...KAI\\ PA/NTA TA\\}\nHEB:['A)NAQEMATIEI', 'AU)TO\\\\N})T', 'KL', ')$R']\nTC: =S"
     ]
    }
   ],
   "source": [
    "files = sorted(glob.glob('source/parallel/*.par')) # all parallel data\n",
    "#files = ['source/parallel/01.Genesis.par'] # load only practice file for now\n",
    "\n",
    "\n",
    "# line numbers to slot numbers\n",
    "greekLine_slots = {} \n",
    "hebrewLine_slots = {}\n",
    "\n",
    "# increment as matches are found\n",
    "catss_slot = 1\n",
    "etcbc_slot = 1\n",
    "\n",
    "# process each file\n",
    "for file in files:\n",
    "    \n",
    "    # open and load file data\n",
    "    with open(file, 'r') as infile:\n",
    "        file_data = infile.read().split('\\n')\n",
    "    \n",
    "    # make the links\n",
    "    skip_line = False\n",
    "    for i, line in enumerate(file_data): \n",
    "        \n",
    "        # handle \"#\" indicator, means data continues to next line\n",
    "        if '#' in line and not skip_line:\n",
    "            line = line.strip() + file_data[i+1].strip() # add next line\n",
    "            line = re.sub('\\t#$|##\\t', '', line).replace('#', '') # remove unwanted chars\n",
    "            skip_line = True \n",
    "        elif '#' in line and skip_line: # skip over line that's been gathered\n",
    "            skip_line = False\n",
    "            continue\n",
    "        \n",
    "        # -- PROCESS 1: CONNECT PARALLEL FILE DATA TO BHSA/CATSS TF SLOTS --\n",
    "         \n",
    "        data = [dat for dat in line.split('\\t') if dat] # Heb/Gre. data tab separated\n",
    "\n",
    "        # skip lines without relevant data\n",
    "        if len(data) < 2:\n",
    "            continue\n",
    "        elif len(data) > 2:\n",
    "            raise Exception(f'>2 tabs in {file} {i+1}\\n{data}\\nraw: {file_data[i]+file_data[i+1]}') # sanity check\n",
    "\n",
    "        hebrew, greek = data # 2 element list\n",
    "\n",
    "        # process text-critical notations first\n",
    "        # some t.c. notations are blended with the surface text\n",
    "        \n",
    "        # separate t.c. notations\n",
    "        c_hebrew = re.sub(tchars, '', hebrew).replace('*', '').split()\n",
    "        c_greek =  re.sub(tcharsG, '', greek).split()\n",
    "        hebrew_tc = ' '.join([tc.strip() for tc in re.findall(tchars, hebrew)])\n",
    "        greek_tc = ' '.join([tc.strip() for tc in re.findall(tcharsG, greek)])\n",
    "        \n",
    "        \n",
    "        if False:\n",
    "            if hebrew_tc and c_hebrew:\n",
    "                \n",
    "                # print tests\n",
    "                print(i+1)\n",
    "                print('SRC:', hebrew)\n",
    "                print('HEB: ', c_hebrew)\n",
    "                print('TC: ', hebrew_tc or 'NONE')\n",
    "                print()\n",
    "                \n",
    "        if False:\n",
    "            if greek_tc and c_greek:\n",
    "                # print tests\n",
    "                print(i+1)\n",
    "                print('SRC:', greek)\n",
    "                print('GRE: ', c_greek)\n",
    "                print('TC: ', greek_tc or 'NONE')\n",
    "                print()\n",
    "\n",
    "        #if i > 2000:\n",
    "        #    break\n",
    "                \n",
    "                \n",
    "        if c_hebrew:\n",
    "            for word in c_hebrew:\n",
    "                if not re.findall(hwords, word):\n",
    "                    raise Exception(f'{file} – {i+1}\\nSRC: {line}\\nHEB:{c_hebrew}\\nTC: {hebrew_tc}')\n",
    "            \n",
    "        if c_greek:\n",
    "            for word in c_greek:\n",
    "                if not re.findall(gwords, word):\n",
    "                    raise Exception(f'{file} – {i+1}\\nSRC: {line}\\nGRE:{c_greek}\\nTC: {greek_tc}')\n",
    "        \n",
    "        \n",
    "        # process Hebrew words\n",
    "        hebrew_words = [subword for word in re.findall(hwords, hebrew)\n",
    "                            for subword in word.strip().replace('*', '').split('/')]                                \n",
    "\n",
    "        # < INSERT BHSA TF SLOT MATCHING>\n",
    "        \n",
    "        # process Greek words\n",
    "        \n",
    "        greek_words = [word.strip() for word in re.findall(gwords, greek)]    \n",
    "\n",
    "        # < INSERT CATSS TF SLOT MATCHING>\n",
    "        \n",
    "        # test prints\n",
    "        #print(i+1)\n",
    "        #print(hebrew)\n",
    "        #print(hebrew_words)\n",
    "        #print()\n",
    "\n",
    "        #if i > 24:\n",
    "            #break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
