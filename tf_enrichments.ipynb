{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATSS Text-Fabric Enrichments\n",
    "\n",
    "The raw data from the CCAT resource has been converted into a TF format in `tf_conversion.ipynb`. This notebook enriches the dataset in a number of ways:\n",
    "\n",
    "* Morphological tags are split and parsed into individual word-level features.\n",
    "* Word-level plain-text is processed into a UTF8 representation feature.\n",
    "* A first effort is made to connect the ETCBC BHSA Hebrew database with the CATSS Hebrew parallel text using parts of speech for word-level connections. Phrase-level connections are created based on phrases' presence per line.\n",
    "* Some rudimentary phrase divisions are also added the CATSS data based on the parallel data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections, re, Levenshtein\n",
    "from tf.fabric import Fabric\n",
    "from greekutils import beta2unicode # do: pip install greek-utils==0.2\n",
    "\n",
    "# some cells are blocked from being run/\n",
    "# toggle True to allow a given task\n",
    "run_corrections = False\n",
    "run_morphology = False\n",
    "run_unicode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrections\n",
    "\n",
    "Corrections to the data based on the decoding processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_corrections toggled False. Doing nothing...\n"
     ]
    }
   ],
   "source": [
    "metadata = metadata = {\n",
    "\n",
    "'': {            \n",
    "        'created_by':'R. Kraft et al., CCAT, University of Pennsylvania',\n",
    "        'converted_by':'Cody Kingham',\n",
    "        'source':'http://ccat.sas.upenn.edu/rak//catss.html',\n",
    "        'license':'http://ccat.sas.upenn.edu/gopher/text/religion/biblical/lxxmorph/0-user-declaration.txt',\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# TO FIX, 2017-12-11:\n",
    "# advb - 173450 (D.P.E)PA/NW) 1 == styp == D.P\n",
    "# 5058 & 5159 (N.N.M.*MESRAIM) 0-2 == styp == N.N.M.\n",
    "\n",
    "run_corrections = False\n",
    "\n",
    "if run_corrections:\n",
    "\n",
    "    # instantiate/load old data\n",
    "    TF = Fabric(locations=['tf'], modules=[''])\n",
    "    api = TF.load('book chapter verse morph trans', silent=True)\n",
    "    api.makeAvailableIn(globals())\n",
    "    \n",
    "    nodes = collections.defaultdict(dict)\n",
    "\n",
    "    # prime metadata\n",
    "    metadata['morph'] = {'valueType': 'str'}\n",
    "    \n",
    "    # corrections for morph feature\n",
    "    for word in F.otype.s('word'):\n",
    "\n",
    "        if word == 11436:\n",
    "            new_morph = 'VA.AAD2P.I(/ZW.KATA'\n",
    "            nodes['morph'][word] = new_morph\n",
    "        else:\n",
    "            nodes['morph'][word] = F.morph.v(word)\n",
    "\n",
    "    save_TF = Fabric(locations=['tf'], modules=['enrichments'])\n",
    "    save_TF.save(nodeFeatures=nodes, metaData=metadata)\n",
    "    del metadata['morph'] # clean out metadata dict\n",
    "    \n",
    "    print('corrections done!')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('run_corrections toggled False. Doing nothing...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Enriched, Corrected CATSS Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.1.1\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "22 features found and 1 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.00s B book                 from tf\n",
      "   |     0.00s B chapter              from tf\n",
      "   |     0.01s B verse                from tf\n",
      "   |     0.19s B trans                from tf\n",
      "   |     0.22s B morph                from tf/enrichments\n",
      "   |     1.60s T typ                  from tf/enrichments\n",
      "   |     0.00s Feature overview: 20 for nodes; 1 for edges; 1 configs; 7 computed\n",
      "  2.57s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "# load in enriched data\n",
    "TF = Fabric(locations=['tf'], modules=['', 'enrichments'])\n",
    "api = TF.load('book chapter verse morph trans typ')\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate New Morphology Features\n",
    "\n",
    "See the [CATSS morphology documentation](http://ccat.sas.upenn.edu/gopher/text/religion/biblical/lxxmorph/*Morph-Coding). In the source data, morphology is space-separated. In the TF version they are dot separated. Tags have to be split, recognized, and converted. They are added as separate word-level features.\n",
    "\n",
    "Morphology codes have 3 or 2 columns, depending on part of speech type. From the documentation:\n",
    "\n",
    "> 1. \"TYPE\" CODES (3 columns maximum, to identify part of speech)\n",
    "> 2. \"PARSE\" CODE (up to 6 columns, as needed, to parse each form) [\\*OPTIONAL]\n",
    "> 3. [lexeme]\n",
    "\n",
    "Proposals for new features:\n",
    "\n",
    "* typ — part of speech, derived from the type codes. It is the first letter of the type code and can have a values of (bold is proposed new feature name):\n",
    "    * N — noun — **noun**\n",
    "    * A — adjective — **adjv**\n",
    "    * R — pronoun — **pron**\n",
    "    * C — conjunction — **conj**\n",
    "    * X — particle — **part**\n",
    "    * I — interjection — **intj**\n",
    "    * M — indeclinable number — **inum**\n",
    "    * P — preposition — **prep**\n",
    "    * D — adverb — **advb**\n",
    "* **styp** — subtype of part of speech, e.g. 1st declension, 3rd declension of various stems. There are lots of categories. The simple code is preserved. Refer to the documentation for their meanings. For that code, I preserve also the part of speech value (N, A, R, etc.)\n",
    "\n",
    "* case — **case**\n",
    "* gender — **gender**\n",
    "* number — **number**\n",
    "* tense — **tense**\n",
    "* voice — **voice**\n",
    "* mood — **mood**\n",
    "* person — **person**\n",
    "* degree — **degree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_morphology toggled False. Doing nothing...\n"
     ]
    }
   ],
   "source": [
    "# Notes to myself\n",
    "# prototypical counts per type:\n",
    "# 3 - adjv, noun, verb\n",
    "# 2 - advb, conj, intj, part, prep, inum\n",
    "# 3 - inum, pron, propn\n",
    "# 4 - propn (N.N.M.MESRAIM), verb (participle)\n",
    "\n",
    "# those with overloaded lexemes:\n",
    "# verb(>3, not participle), verb(>4, participle)\n",
    "\n",
    "# store new features here: feature_name to node to feature \n",
    "features = collections.defaultdict(dict)\n",
    "\n",
    "# conversion dicts\n",
    "typs = {'N': 'noun',\n",
    "        'V': 'verb',\n",
    "        'A': 'adjv',\n",
    "        'R': 'pron',\n",
    "        'C': 'conj',\n",
    "        'X': 'part',\n",
    "        'I': 'intj',\n",
    "        'M': 'inum',\n",
    "        'P': 'prep',\n",
    "        'D': 'advb'}\n",
    "       #'N': 'propn' proper noun, added below with special rule\n",
    "    \n",
    "# nominals \n",
    "# [case][number][gender]\n",
    "cases = {'N': 'nom',\n",
    "         'G': 'gen',\n",
    "         'D': 'dat',\n",
    "         'A': 'acc',\n",
    "         'V': 'voc'}\n",
    "numbers = {'S': 'sg',\n",
    "          'D': 'du',\n",
    "          'P': 'pl'}\n",
    "genders = {'M': 'm',\n",
    "          'F': 'f',\n",
    "          'N': 'n'}\n",
    "degrees = {'C': 'comparative',\n",
    "          'S': 'superlative'}\n",
    "\n",
    "# verbs\n",
    "# [tense][voice][mood][person][number] [case][number][gender]\n",
    "\n",
    "tenses = {'P': 'present',\n",
    "         'I': 'imperfect',\n",
    "         'F': 'future',\n",
    "         'A': 'aorist',\n",
    "         'X': 'perfect',\n",
    "         'Y': 'pluperfect'}\n",
    "voices = {'A': 'active',\n",
    "         'M': 'middle',\n",
    "         'P': 'passsive'}\n",
    "moods = {'I': 'indc',\n",
    "         'D': 'impv',\n",
    "         'S': 'subj',\n",
    "         'O': 'optv',\n",
    "         'N': 'infv',\n",
    "         'P': 'ptcp'}\n",
    "\n",
    "if run_morphology:\n",
    "\n",
    "    # big loop\n",
    "    for word in F.otype.s('word'):\n",
    "\n",
    "        morph = F.morph.v(word)\n",
    "        split_morph = morph.split('.')\n",
    "\n",
    "        # parse morphology codes in order of appearance:\n",
    "\n",
    "\n",
    "        # 1. assign subtypes and types\n",
    "\n",
    "        styp = split_morph[0] # subtype\n",
    "\n",
    "        # get type; exception for proper nouns; nouns with no subtypes\n",
    "        if styp == 'N':\n",
    "            typ = 'propn'\n",
    "        else:\n",
    "            typ = typs[styp[0]] # type is only first char of code, convert it\n",
    "\n",
    "\n",
    "        # 2. assign parsing data\n",
    "\n",
    "        # indeclinable words\n",
    "        if len(split_morph) == 2 or typ in {'advb', 'conj'}:\n",
    "            case, gender, number, degree, tense, voice, mood, person = ('' for i in range(1,9))\n",
    "            lexeme = '.'.join(split_morph[1:])\n",
    "\n",
    "\n",
    "        # nominal words with case/gender/number\n",
    "        elif typ in {'adjv', 'noun', 'inum', 'pron', 'propn'}:\n",
    "\n",
    "            parsing_data = split_morph[1]\n",
    "            case = ''\n",
    "            gender = ''\n",
    "            number = ''\n",
    "            degree = ''\n",
    "\n",
    "            # get parsing; some parsing codes have < 3 values, loop is thus necessary\n",
    "            for i, char in enumerate(parsing_data):\n",
    "\n",
    "                # dative/dual disambiguation\n",
    "                if i == 0 and char == 'D': \n",
    "                    case = 'dat'\n",
    "                elif i != 0 and char == 'D':\n",
    "                    number = 'du'\n",
    "\n",
    "                # disambiguation for 'S' superlative\n",
    "                elif all([char == 'S' or char == 'C', len(parsing_data) == 4,\n",
    "                          typ == 'adjv', i != 1]):\n",
    "                    degree = degrees.get(char, '')\n",
    "\n",
    "                # all other parsings\n",
    "                elif char != 'D':\n",
    "                    case = cases.get(char, '') if not case else case\n",
    "                    gender = genders.get(char, '') if not gender else gender\n",
    "                    number = numbers.get(char, '') if not number else number\n",
    "                    degree = '' if not degree else degree\n",
    "\n",
    "            # set non applicable values to null\n",
    "            person, tense, voice, mood = ('' for i in range(1,5))\n",
    "\n",
    "            lexeme = '.'.join(split_morph[2:])\n",
    "\n",
    "\n",
    "        # verbs\n",
    "        elif typ == 'verb':\n",
    "\n",
    "            parsing_data = split_morph[1]\n",
    "            tense = tenses[parsing_data[0] ]\n",
    "            try:\n",
    "                voice = voices[parsing_data[1]]\n",
    "\n",
    "            except:\n",
    "                print(word, morph)\n",
    "            mood = moods[parsing_data[2]]\n",
    "\n",
    "            # handle participles \n",
    "            try:\n",
    "                gender = genders[parsing_data[5]] # only participles have >4 chars\n",
    "                number = numbers[parsing_data[4]]\n",
    "                case = cases[parsing_data[3]]\n",
    "                person = '' # non-applicable values\n",
    "                degree = ''\n",
    "\n",
    "            except IndexError:\n",
    "\n",
    "                # all normal verbs\n",
    "                try:\n",
    "                    person = parsing_data[3]\n",
    "                    number = numbers[parsing_data[4]]\n",
    "                    case = '' # non-applicable values\n",
    "                    gender = '' \n",
    "                    degree = ''\n",
    "\n",
    "                # handle infinitives\n",
    "                except IndexError: \n",
    "                    person = '' # non-applicable values\n",
    "                    number = ''\n",
    "                    case = ''\n",
    "                    gender = ''\n",
    "                    degree = ''\n",
    "\n",
    "            lexeme = '.'.join(split_morph[2:])\n",
    "\n",
    "        # assign features\n",
    "        features['typ'][word] = typ\n",
    "        features['styp'][word] = styp\n",
    "        features['lex'][word] = lexeme\n",
    "        features['case'][word] = case\n",
    "        features['number'][word] = number\n",
    "        features['gender'][word] = gender\n",
    "        features['degree'][word] = degree\n",
    "        features['tense'][word] = tense\n",
    "        features['voice'][word] = voice\n",
    "        features['mood'][word] = mood\n",
    "        features['person'][word] = person\n",
    "\n",
    "    print(f'done! with {len(features)} new features logged...')\n",
    "    \n",
    "    # EXPORT new features\n",
    "    \n",
    "    for feat in features:\n",
    "        metadata[feat] = {'valueType': 'str'}\n",
    "    \n",
    "    TF.save(nodeFeatures=features, metaData=metadata)\n",
    "    \n",
    "else:\n",
    "    print('run_morphology toggled False. Doing nothing...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate UTF8 Feature\n",
    "\n",
    "The transcription table is available at [the CCAT documentation page](http://ccat.sas.upenn.edu/gopher/text/religion/biblical/lxxmorph/0-betacode.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_unicode toggled False. Doing nothing...\n"
     ]
    }
   ],
   "source": [
    "if run_unicode:\n",
    "    \n",
    "    for word in F.otype.s('word'):\n",
    "\n",
    "        unicode = beta2unicode.convert(F.trans.v(word))\n",
    "\n",
    "        # add final sigma fix!\n",
    "        if unicode[-1] == 'σ':\n",
    "            unicode = unicode[:-1] + 'ς' \n",
    "            \n",
    "        # save feature\n",
    "        features['utf8'][word] = unicode\n",
    "            \n",
    "    # export unicode features\n",
    "    for feat in features:\n",
    "        metadata[feat] = {'valueType': 'str'}\n",
    "    \n",
    "    TF.save(nodeFeatures=features, metaData=metadata)\n",
    "    \n",
    "else: \n",
    "    print('run_unicode toggled False. Doing nothing...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Integration of Parallel Data with ETCBC Hebrew Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.1.1\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "114 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.11s B g_cons               from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s Feature overview: 108 for nodes; 5 for edges; 1 configs; 7 computed\n",
      "  3.93s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "# instantiate ETCBC data\n",
    "\n",
    "hebrew_etcbc = Fabric(locations='~/github/etcbc/bhsa/tf', modules='c')\n",
    "etcbc = hebrew_etcbc.load('g_cons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23583"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccat_etcbc = {')': '>',\n",
    "             'B': 'B',\n",
    "             'G': 'G',\n",
    "             'D': 'D',\n",
    "             'H': 'H',\n",
    "             'W': 'W',\n",
    "             'Z': 'Z',\n",
    "             'X': 'X',\n",
    "             '+': 'V',\n",
    "             'Y': 'J',\n",
    "             'K': 'K',\n",
    "             'L': 'L',\n",
    "             'M': 'M',\n",
    "             'N': 'N',\n",
    "             'S': 'S',\n",
    "             '(': '<',\n",
    "             'P': 'P',\n",
    "             'C': 'Y',\n",
    "             'Q': 'Q',\n",
    "             'R': 'R',\n",
    "             '&': 'F',\n",
    "             '$': 'C',\n",
    "             'T': 'T',\n",
    "             }\n",
    "\n",
    "def convert_ccat(string):\n",
    "    \n",
    "    '''\n",
    "    Simply assembles and returns an ETCBC transcription string\n",
    "    from the CCAT Hebrew transcription.\n",
    "    '''\n",
    "    \n",
    "    converted = ''.join(ccat_etcbc[char] if char in ccat_etcbc else char\n",
    "                            for char in string)\n",
    "    \n",
    "    return converted\n",
    "    \n",
    "def is_match(hebrew_list, greek_list, tolerance=2):\n",
    "    \n",
    "    '''\n",
    "    Match two supplied lists of surface forms.\n",
    "    Returns boolean.\n",
    "    tolerance defines the number of characters that can differ per word.\n",
    "    '''\n",
    "    \n",
    "    # check word by word\n",
    "    for hebrew_w, greek_w in zip(hebrew_list, greek_list):\n",
    "        if Levenshtein.distance(hebrew_w, greek_w) > tolerance:\n",
    "            return False\n",
    "    # true if it reaches this point\n",
    "    return True\n",
    "\n",
    "file = 'source/parallel/01.Genesis.par'\n",
    "\n",
    "with open(file, 'r') as infile:\n",
    "    \n",
    "    gen_par = infile.read().split('\\n')\n",
    "    \n",
    "len(gen_par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Integration\n",
    "\n",
    "The connections are made with two processes.\n",
    "\n",
    "The first process involves matching the data in the CATSS parallel files to their respective databases. The connections are made on the basis of matching surface forms (within 1 character, using Levenshtein distance). Once matched, the relevant slot data is dropped into a tuple which is in turn keyed to its line number in the dictionaries `greekLine_slots` and `hebrewLine_slots` (to be accessed in process 2).\n",
    "\n",
    "The second process involves matching the Hebrew and Greek language data using the tuples stored in process 1. Those matches are made based on matching part of speech tags. The matches are made in sequence to avoid mismatching multiple part of speech tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "B/R)$YT\n",
      "['B', 'R)$YT']\n",
      "\n",
      "2\n",
      "BR)\n",
      "['BR)']\n",
      "\n",
      "3\n",
      ")LHYM\n",
      "[')LHYM']\n",
      "\n",
      "4\n",
      ")T H/$MYM\n",
      "[')T', 'H', '$MYM']\n",
      "\n",
      "5\n",
      "W/)T H/)RC\n",
      "['W', ')T', 'H', ')RC']\n",
      "\n",
      "8\n",
      "W/H/)RC\n",
      "['W', 'H', ')RC']\n",
      "\n",
      "9\n",
      "HYTH\n",
      "['HYTH']\n",
      "\n",
      "10\n",
      "THW\n",
      "['THW']\n",
      "\n",
      "11\n",
      "W/BHW\n",
      "['W', 'BHW']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "greekLine_slots = {} # line numbers to slot numbers\n",
    "hebrewLine_slots = {}\n",
    "\n",
    "# for matching words\n",
    "hchars = ''.join(ccat_etcbc.keys()) # string of hebrew characters in transcription\n",
    "gchars = ''.join(set(l for w in F.otype.s('word') # string of greek characters in transcription\n",
    "                       for l in F.trans.v(w)\n",
    "                    ))\n",
    "gchars = gchars.replace('\\\\', '\\\\\\\\').replace('-', '\\-') # fix escape bug with re\n",
    "\n",
    "catss_slot = 1\n",
    "etcbc_slot = 1\n",
    "\n",
    "for i, line in enumerate(gen_par):\n",
    "    \n",
    "    # hebrew/greek data is tab separated; split and store\n",
    "    data = [dat for dat in gen_par[i].split('\\t') if dat]\n",
    "        \n",
    "    # skip lines without relevant data\n",
    "    if len(data) != 2:\n",
    "        continue\n",
    "        \n",
    "    # split language strings into word strings\n",
    "    hebrew = data[0]\n",
    "    greek = data[1]\n",
    "    \n",
    "    # process Hebrew; regex pattern + list comprehension = clean matched words\n",
    "    heb_pattern = f'^\\*?[/{hchars}]+|\\s\\*?[/{hchars}]+'\n",
    "    hebrew_words = [subword for word in re.findall(heb_pattern, hebrew)\n",
    "                        for subword in word.strip().replace('*', '').split('/')]                                   \n",
    "    \n",
    "    # process text-critical notations\n",
    "    # for Hebrew it is fairly straightforward: apply the t.c. regex pattern\n",
    "    # for Greek there is crossover with Greek transcription:\n",
    "    #    so t.c. matches must be excluded from Greek word matches\n",
    "    \n",
    "    \n",
    "    # process Greek\n",
    "    gre_pattern = f'^[{gchars}]*|\\s[{gchars}]*'\n",
    "    greek_words = [word.strip() for word in re.findall(gre_pattern, greek)]    \n",
    "\n",
    "    \n",
    "    # test prints\n",
    "    print(i)\n",
    "    print(hebrew)\n",
    "    print(hebrew_words)\n",
    "    print()\n",
    "    \n",
    "    if i > 10:\n",
    "        break\n",
    "    \n",
    "    \n",
    "    # temporary structure for testing; remove try/except later\n",
    "'''    try:\n",
    "        ccat_words = [convert_ccat(w) for w in hebrew] # convert ccat words to etcbc transcription\n",
    "    except:\n",
    "\n",
    "        if re.findall(tc_signals, hebrew[1]):\n",
    "            hebrew.remove(hebrew[1])\n",
    "            ccat_words = ccat_words = [convert_ccat(w) for w in hebrew]\n",
    "\n",
    "        #raise Exception(f'line: {i}\\n hebrew: {hebrew}\\n etcbc_slot: {etcbc_slot}')\n",
    "\n",
    "\n",
    "    etcbc_words = []\n",
    "    etcbc_slots = []\n",
    "    for word in hebrew:\n",
    "\n",
    "        if not etcbc.F.g_cons.v(etcbc_slot):\n",
    "            etcbc_slot += 1\n",
    "\n",
    "        etcbc_words.append(etcbc.F.g_cons.v(etcbc_slot))\n",
    "        etcbc_slots.append(etcbc_slot)\n",
    "        etcbc_slot += 1\n",
    "\n",
    "\n",
    "    if etcbc_words != ccat_words:\n",
    "        raise Exception(f'line: {i} \\n hebrew: {hebrew} \\n etcbc: {etcbc_words} \\n etcbc.slots: {etcbc_slots} ')\n",
    "\n",
    "    print(i, ccat_words)\n",
    "    print(etcbc_words)\n",
    "    print('\\t\\t', etcbc_slots)\n",
    "    print()'''\n",
    "\n",
    "man = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['**)RC']"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc_sigla = {\n",
    "            '{\\.\\.\\.}',\n",
    "            \n",
    "            '~', # differences in HB sequence, investigate\n",
    "            '~~~',\n",
    "            '{\\.\\.~}',\n",
    "            \n",
    "            \"''\", # long minus/plus with quotes\n",
    "            '{d}', '{\\.\\.d}', '{\\.\\.r}',\n",
    "            '\\?',\n",
    "            '{p}',\n",
    "            '' # infinitive absolute + data\n",
    "           }\n",
    "\n",
    "# to find and add:\n",
    "# ~, ~~~\n",
    "            \n",
    "# tc characters w/ formatting:\n",
    "tchar_form = {'\\*\\*[{hchars}]+':'Qere',\n",
    "              '\\*z[{hchars}]+': 'Qere wela ketib, ketib wela qere.',\n",
    "             }\n",
    "              \n",
    "    \n",
    "# all other tc chars\n",
    "tchar_norm = {'{#}':,\n",
    "              '{g}':,\n",
    "              '{\\.\\.\\.}':,\n",
    "              '\\.\\.a':,\n",
    "              '\\[\\d*\\]':\n",
    "                  'Reference of number of verse in LXX, different from MT.',\n",
    "              '\\[\\[\\d*\\]\\]':\n",
    "                  'Reference number of verse in MT, different from the LXX.',\n",
    "              '---':\n",
    "                  'Apparent minus',\n",
    "              '--\\+':\n",
    "                  'Apparent plus created by lack of equivalence between long stretches of text in the LXX and MT.',\n",
    "              '\\'\\'':\n",
    "                  'Long minus or plus (at least four lines).',\n",
    "              '{\\.\\.\\.}':\n",
    "                  'Equivalent reflected elsewhere in the text, disregarded by indexing program.',\n",
    "              '{\\.\\.~}':'Stylistic or grammatical transposition.',\n",
    "              '{d}':'Reference to doublet (occurring between the two elements of the doublet.',\n",
    "              '{\\.\\.d}': 'Distributive rendering, occurring once in the translation but referring to more than one Hebrew word.',\n",
    "              '{\\.\\.r}': 'Notation in Hebrew column of elements repeated in the translation.',\n",
    "              '.*\\?.*': 'Questionable notation, equivalent, etc.'\n",
    "              '{p}':'Greek preverb representing Hebrew preposition.',\n",
    "              '{\\..\\p}': 'Preposition added in the LXX in accordance with the rules of the Greek language or translational habits.',\n",
    "              '{!}[a-z-]*': 'Infinitive absolute',\n",
    "              ''\n",
    "             }\n",
    "# combined tc chars\n",
    "tchar = '|'.join(tchar_norm) + f'|'.join(tchar_form)\n",
    "\n",
    "\n",
    "\n",
    "tchars = '' + tchars # strings with braces {}\n",
    "\n",
    "test = '**)RC'\n",
    "            \n",
    "re.findall(tchars, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\*\\*[)BGDHWZX+YKLMNS(PCQR&$T]*|\n"
     ]
    }
   ],
   "source": [
    "print(tchars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{!}ad']"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple regex tests \n",
    "\n",
    "re.findall('{!}[a-z-]*', '{!}ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
