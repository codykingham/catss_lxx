{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATSS Text-Fabric Enrichments\n",
    "\n",
    "The raw data from the CCAT resource has been converted into a TF format in `tf_conversion.ipynb`. This notebook enriches the dataset in a number of ways:\n",
    "\n",
    "* Morphological tags are split and parsed into individual word-level features.\n",
    "* Word-level plain-text is processed into a UTF8 representation feature.\n",
    "* A first effort is made to connect the ETCBC BHSA Hebrew database with the CATSS Hebrew parallel text using parts of speech for word-level connections. Phrase-level connections are created based on phrases' presence per line.\n",
    "* Some rudimentary phrase divisions are also added the CATSS data based on the parallel data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections, re \n",
    "from tf.fabric import Fabric\n",
    "from greekutils import beta2unicode # do: pip install greek-utils==0.2\n",
    "\n",
    "# some cells are blocked from being run/\n",
    "# toggle True to allow a given task\n",
    "run_corrections = False\n",
    "run_morphology = False\n",
    "run_unicode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrections\n",
    "\n",
    "Corrections to the data based on the decoding processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_corrections toggled False. Doing nothing...\n"
     ]
    }
   ],
   "source": [
    "metadata = metadata = {\n",
    "\n",
    "'': {            \n",
    "        'created_by':'R. Kraft et al., CCAT, University of Pennsylvania',\n",
    "        'converted_by':'Cody Kingham',\n",
    "        'source':'http://ccat.sas.upenn.edu/rak//catss.html',\n",
    "        'license':'http://ccat.sas.upenn.edu/gopher/text/religion/biblical/lxxmorph/0-user-declaration.txt',\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# TO FIX, 2017-12-11:\n",
    "# advb - 173450 (D.P.E)PA/NW) 1 == styp == D.P\n",
    "# 5058 & 5159 (N.N.M.*MESRAIM) 0-2 == styp == N.N.M.\n",
    "\n",
    "run_corrections = False\n",
    "\n",
    "if run_corrections:\n",
    "\n",
    "    # instantiate/load old data\n",
    "    TF = Fabric(locations=['tf'], modules=[''])\n",
    "    api = TF.load('book chapter verse morph trans', silent=True)\n",
    "    api.makeAvailableIn(globals())\n",
    "    \n",
    "    nodes = collections.defaultdict(dict)\n",
    "\n",
    "    # prime metadata\n",
    "    metadata['morph'] = {'valueType': 'str'}\n",
    "    \n",
    "    # corrections for morph feature\n",
    "    for word in F.otype.s('word'):\n",
    "\n",
    "        if word == 11436:\n",
    "            new_morph = 'VA.AAD2P.I(/ZW.KATA'\n",
    "            nodes['morph'][word] = new_morph\n",
    "        else:\n",
    "            nodes['morph'][word] = F.morph.v(word)\n",
    "\n",
    "    save_TF = Fabric(locations=['tf'], modules=['enrichments'])\n",
    "    save_TF.save(nodeFeatures=nodes, metaData=metadata)\n",
    "    del metadata['morph'] # clean out metadata dict\n",
    "    \n",
    "    print('corrections done!')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('run_corrections toggled False. Doing nothing...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Enriched, Corrected CATSS Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.1.1\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "22 features found and 1 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.00s B book                 from tf\n",
      "   |     0.00s B chapter              from tf\n",
      "   |     0.01s B verse                from tf\n",
      "   |     0.18s B trans                from tf\n",
      "   |     0.22s B morph                from tf/enrichments\n",
      "   |     0.00s Feature overview: 20 for nodes; 1 for edges; 1 configs; 7 computed\n",
      "  0.96s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "# load in enriched data\n",
    "TF = Fabric(locations=['tf'], modules=['', 'enrichments'])\n",
    "api = TF.load('book chapter verse morph trans')\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate New Morphology Features\n",
    "\n",
    "See the [CATSS morphology documentation](http://ccat.sas.upenn.edu/gopher/text/religion/biblical/lxxmorph/*Morph-Coding). In the source data, morphology is space-separated. In the TF version they are dot separated. Tags have to be split, recognized, and converted. They are added as separate word-level features.\n",
    "\n",
    "Morphology codes have 3 or 2 columns, depending on part of speech type. From the documentation:\n",
    "\n",
    "> 1. \"TYPE\" CODES (3 columns maximum, to identify part of speech)\n",
    "> 2. \"PARSE\" CODE (up to 6 columns, as needed, to parse each form) [\\*OPTIONAL]\n",
    "> 3. [lexeme]\n",
    "\n",
    "Proposals for new features:\n",
    "\n",
    "* typ — part of speech, derived from the type codes. It is the first letter of the type code and can have a values of (bold is proposed new feature name):\n",
    "    * N — noun — **noun**\n",
    "    * A — adjective — **adjv**\n",
    "    * R — pronoun — **pron**\n",
    "    * C — conjunction — **conj**\n",
    "    * X — particle — **part**\n",
    "    * I — interjection — **intj**\n",
    "    * M — indeclinable number — **inum**\n",
    "    * P — preposition — **prep**\n",
    "    * D — adverb — **advb**\n",
    "* **styp** — subtype of part of speech, e.g. 1st declension, 3rd declension of various stems. There are lots of categories. The simple code is preserved. Refer to the documentation for their meanings. For that code, I preserve also the part of speech value (N, A, R, etc.)\n",
    "\n",
    "* case — **case**\n",
    "* gender — **gender**\n",
    "* number — **number**\n",
    "* tense — **tense**\n",
    "* voice — **voice**\n",
    "* mood — **mood**\n",
    "* person — **person**\n",
    "* degree — **degree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_morphology toggled False. Doing nothing...\n"
     ]
    }
   ],
   "source": [
    "# Notes to myself\n",
    "# prototypical counts per type:\n",
    "# 3 - adjv, noun, verb\n",
    "# 2 - advb, conj, intj, part, prep, inum\n",
    "# 3 - inum, pron, propn\n",
    "# 4 - propn (N.N.M.MESRAIM), verb (participle)\n",
    "\n",
    "# those with overloaded lexemes:\n",
    "# verb(>3, not participle), verb(>4, participle)\n",
    "\n",
    "# store new features here: feature_name to node to feature \n",
    "features = collections.defaultdict(dict)\n",
    "\n",
    "# conversion dicts\n",
    "typs = {'N': 'noun',\n",
    "        'V': 'verb',\n",
    "        'A': 'adjv',\n",
    "        'R': 'pron',\n",
    "        'C': 'conj',\n",
    "        'X': 'part',\n",
    "        'I': 'intj',\n",
    "        'M': 'inum',\n",
    "        'P': 'prep',\n",
    "        'D': 'advb'}\n",
    "       #'N': 'propn' proper noun, added below with special rule\n",
    "    \n",
    "# nominals \n",
    "# [case][number][gender]\n",
    "cases = {'N': 'nom',\n",
    "         'G': 'gen',\n",
    "         'D': 'dat',\n",
    "         'A': 'acc',\n",
    "         'V': 'voc'}\n",
    "numbers = {'S': 'sg',\n",
    "          'D': 'du',\n",
    "          'P': 'pl'}\n",
    "genders = {'M': 'm',\n",
    "          'F': 'f',\n",
    "          'N': 'n'}\n",
    "degrees = {'C': 'comparative',\n",
    "          'S': 'superlative'}\n",
    "\n",
    "# verbs\n",
    "# [tense][voice][mood][person][number] [case][number][gender]\n",
    "\n",
    "tenses = {'P': 'present',\n",
    "         'I': 'imperfect',\n",
    "         'F': 'future',\n",
    "         'A': 'aorist',\n",
    "         'X': 'perfect',\n",
    "         'Y': 'pluperfect'}\n",
    "voices = {'A': 'active',\n",
    "         'M': 'middle',\n",
    "         'P': 'passsive'}\n",
    "moods = {'I': 'indc',\n",
    "         'D': 'impv',\n",
    "         'S': 'subj',\n",
    "         'O': 'optv',\n",
    "         'N': 'infv',\n",
    "         'P': 'ptcp'}\n",
    "\n",
    "if run_morphology:\n",
    "\n",
    "    # big loop\n",
    "    for word in F.otype.s('word'):\n",
    "\n",
    "        morph = F.morph.v(word)\n",
    "        split_morph = morph.split('.')\n",
    "\n",
    "        # parse morphology codes in order of appearance:\n",
    "\n",
    "\n",
    "        # 1. assign subtypes and types\n",
    "\n",
    "        styp = split_morph[0] # subtype\n",
    "\n",
    "        # get type; exception for proper nouns; nouns with no subtypes\n",
    "        if styp == 'N':\n",
    "            typ = 'propn'\n",
    "        else:\n",
    "            typ = typs[styp[0]] # type is only first char of code, convert it\n",
    "\n",
    "\n",
    "        # 2. assign parsing data\n",
    "\n",
    "        # indeclinable words\n",
    "        if len(split_morph) == 2 or typ in {'advb', 'conj'}:\n",
    "            case, gender, number, degree, tense, voice, mood, person = ('' for i in range(1,9))\n",
    "            lexeme = '.'.join(split_morph[1:])\n",
    "\n",
    "\n",
    "        # nominal words with case/gender/number\n",
    "        elif typ in {'adjv', 'noun', 'inum', 'pron', 'propn'}:\n",
    "\n",
    "            parsing_data = split_morph[1]\n",
    "            case = ''\n",
    "            gender = ''\n",
    "            number = ''\n",
    "            degree = ''\n",
    "\n",
    "            # get parsing; some parsing codes have < 3 values, loop is thus necessary\n",
    "            for i, char in enumerate(parsing_data):\n",
    "\n",
    "                # dative/dual disambiguation\n",
    "                if i == 0 and char == 'D': \n",
    "                    case = 'dat'\n",
    "                elif i != 0 and char == 'D':\n",
    "                    number = 'du'\n",
    "\n",
    "                # disambiguation for 'S' superlative\n",
    "                elif all([char == 'S' or char == 'C', len(parsing_data) == 4,\n",
    "                          typ == 'adjv', i != 1]):\n",
    "                    degree = degrees.get(char, '')\n",
    "\n",
    "                # all other parsings\n",
    "                elif char != 'D':\n",
    "                    case = cases.get(char, '') if not case else case\n",
    "                    gender = genders.get(char, '') if not gender else gender\n",
    "                    number = numbers.get(char, '') if not number else number\n",
    "                    degree = '' if not degree else degree\n",
    "\n",
    "            # set non applicable values to null\n",
    "            person, tense, voice, mood = ('' for i in range(1,5))\n",
    "\n",
    "            lexeme = '.'.join(split_morph[2:])\n",
    "\n",
    "\n",
    "        # verbs\n",
    "        elif typ == 'verb':\n",
    "\n",
    "            parsing_data = split_morph[1]\n",
    "            tense = tenses[parsing_data[0] ]\n",
    "            try:\n",
    "                voice = voices[parsing_data[1]]\n",
    "\n",
    "            except:\n",
    "                print(word, morph)\n",
    "            mood = moods[parsing_data[2]]\n",
    "\n",
    "            # handle participles \n",
    "            try:\n",
    "                gender = genders[parsing_data[5]] # only participles have >4 chars\n",
    "                number = numbers[parsing_data[4]]\n",
    "                case = cases[parsing_data[3]]\n",
    "                person = '' # non-applicable values\n",
    "                degree = ''\n",
    "\n",
    "            except IndexError:\n",
    "\n",
    "                # all normal verbs\n",
    "                try:\n",
    "                    person = parsing_data[3]\n",
    "                    number = numbers[parsing_data[4]]\n",
    "                    case = '' # non-applicable values\n",
    "                    gender = '' \n",
    "                    degree = ''\n",
    "\n",
    "                # handle infinitives\n",
    "                except IndexError: \n",
    "                    person = '' # non-applicable values\n",
    "                    number = ''\n",
    "                    case = ''\n",
    "                    gender = ''\n",
    "                    degree = ''\n",
    "\n",
    "            lexeme = '.'.join(split_morph[2:])\n",
    "\n",
    "        # assign features\n",
    "        features['typ'][word] = typ\n",
    "        features['styp'][word] = styp\n",
    "        features['lex'][word] = lexeme\n",
    "        features['case'][word] = case\n",
    "        features['number'][word] = number\n",
    "        features['gender'][word] = gender\n",
    "        features['degree'][word] = degree\n",
    "        features['tense'][word] = tense\n",
    "        features['voice'][word] = voice\n",
    "        features['mood'][word] = mood\n",
    "        features['person'][word] = person\n",
    "\n",
    "    print(f'done! with {len(features)} new features logged...')\n",
    "    \n",
    "    # EXPORT new features\n",
    "    \n",
    "    for feat in features:\n",
    "        metadata[feat] = {'valueType': 'str'}\n",
    "    \n",
    "    TF.save(nodeFeatures=features, metaData=metadata)\n",
    "    \n",
    "else:\n",
    "    print('run_morphology toggled False. Doing nothing...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate UTF8 Feature\n",
    "\n",
    "The transcription table is available at [the CCAT documentation page](http://ccat.sas.upenn.edu/gopher/text/religion/biblical/lxxmorph/0-betacode.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_unicode toggled False. Doing nothing...\n"
     ]
    }
   ],
   "source": [
    "if run_unicode:\n",
    "    \n",
    "    for word in F.otype.s('word'):\n",
    "\n",
    "        unicode = beta2unicode.convert(F.trans.v(word))\n",
    "\n",
    "        # add final sigma fix!\n",
    "        if unicode[-1] == 'σ':\n",
    "            unicode = unicode[:-1] + 'ς' \n",
    "            \n",
    "        # save feature\n",
    "        features['utf8'][word] = unicode\n",
    "            \n",
    "    # export unicode features\n",
    "    for feat in features:\n",
    "        metadata[feat] = {'valueType': 'str'}\n",
    "    \n",
    "    TF.save(nodeFeatures=features, metaData=metadata)\n",
    "    \n",
    "else: \n",
    "    print('run_unicode toggled False. Doing nothing...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Integration of Parallel Data with ETCBC Hebrew Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.1.1\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "114 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.16s B g_cons               from /Users/cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s Feature overview: 108 for nodes; 5 for edges; 1 configs; 7 computed\n",
      "  3.99s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "# instantiate ETCBC data\n",
    "\n",
    "hebrew_etcbc = Fabric(locations='~/github/etcbc/bhsa/tf', modules='c')\n",
    "etcbc = hebrew_etcbc.load('g_cons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23583"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def convert_ccat(string):\n",
    "    \n",
    "    '''\n",
    "    Simply assembles and returns an ETCBC transcription string\n",
    "    from the CCAT Hebrew transcription.\n",
    "    '''\n",
    "    \n",
    "    ccat_etcbc = {')': '>',\n",
    "                 'B': 'B',\n",
    "                 'G': 'G',\n",
    "                 'D': 'D',\n",
    "                 'H': 'H',\n",
    "                 'W': 'W',\n",
    "                 'Z': 'Z',\n",
    "                 'X': 'X',\n",
    "                 '+': 'V',\n",
    "                 'Y': 'J',\n",
    "                 'K': 'K',\n",
    "                 'L': 'L',\n",
    "                 'M': 'M',\n",
    "                 'N': 'N',\n",
    "                 'S': 'S',\n",
    "                 '(': '<',\n",
    "                 'P': 'P',\n",
    "                 'C': 'Y',\n",
    "                 'Q': 'Q',\n",
    "                 'R': 'R',\n",
    "                 '&': 'F',\n",
    "                 '$': 'C',\n",
    "                 'T': 'T',\n",
    "                 '-': '', # ignore maqqeph\n",
    "                 }\n",
    "    \n",
    "    converted = ''.join(ccat_etcbc[char] for char in string)\n",
    "    \n",
    "    return converted\n",
    "    \n",
    "\n",
    "file = 'source/parallel/01.Genesis.par'\n",
    "\n",
    "with open(file, 'r') as infile:\n",
    "    \n",
    "    gen_par = infile.read().split('\\n')\n",
    "    \n",
    "len(gen_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "line: 160 \n hebrew: ['^', 'L', 'MYN', 'W'] \n etcbc: ['L', 'MJNW', '>CR', 'ZR<W'] \n etcbc.slots: [193, 194, 195, 196] ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-01ab6dc005f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0metcbc_words\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mccat_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'line: {i} \\n hebrew: {hebrew} \\n etcbc: {etcbc_words} \\n etcbc.slots: {etcbc_slots} '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#print(i, ccat_words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: line: 160 \n hebrew: ['^', 'L', 'MYN', 'W'] \n etcbc: ['L', 'MJNW', '>CR', 'ZR<W'] \n etcbc.slots: [193, 194, 195, 196] "
     ]
    }
   ],
   "source": [
    "etcbc_slot = 1\n",
    "\n",
    "for i in range(1,1000):\n",
    "    \n",
    "    line = [data for data in gen_par[i].split('\\t') if data]\n",
    "        \n",
    "    if len(line) == 2:\n",
    "        \n",
    "        hebrew = [w for w in re.split('\\s|/', line[0])]\n",
    "        greek = line[1].split()\n",
    "        \n",
    "        # check for text critical notations\n",
    "        # there is no corresponding value in ETCBC\n",
    "        tc_signals = '-\\+|='\n",
    "        is_tc = re.findall(tc_signals, hebrew[0])\n",
    "        if is_tc:  \n",
    "            # put actions here; for now we skip\n",
    "            continue\n",
    "        \n",
    "        # temporary structure for testing; remove try/except later\n",
    "        try:\n",
    "            ccat_words = [convert_ccat(w) for w in hebrew] # convert ccat words to etcbc transcription\n",
    "        except:\n",
    "            \n",
    "            if re.findall(tc_signals, hebrew[1]):\n",
    "                hebrew.remove(hebrew[1])\n",
    "                ccat_words = ccat_words = [convert_ccat(w) for w in hebrew]\n",
    "            \n",
    "            #raise Exception(f'line: {i}\\n hebrew: {hebrew}\\n etcbc_slot: {etcbc_slot}')\n",
    "\n",
    "        \n",
    "        etcbc_words = []\n",
    "        etcbc_slots = []\n",
    "        for word in hebrew:\n",
    "            \n",
    "            if not etcbc.F.g_cons.v(etcbc_slot):\n",
    "                etcbc_slot += 1\n",
    "            \n",
    "            etcbc_words.append(etcbc.F.g_cons.v(etcbc_slot))\n",
    "            etcbc_slots.append(etcbc_slot)\n",
    "            etcbc_slot += 1\n",
    "        \n",
    "        \n",
    "        if etcbc_words != ccat_words:\n",
    "            raise Exception(f'line: {i} \\n hebrew: {hebrew} \\n etcbc: {etcbc_words} \\n etcbc.slots: {etcbc_slots} ')\n",
    "        \n",
    "        #print(i, ccat_words)\n",
    "        #print(etcbc_words)\n",
    "        #print('\\t\\t', etcbc_slots)\n",
    "        #print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
